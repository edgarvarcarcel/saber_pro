{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1bda80-e609-40ec-a8f0-1b85a58f2b62",
   "metadata": {},
   "source": [
    "# **0. Introduction**\n",
    "This notebook build a GAN, cGAn and Variational AutoEncoder for data augmentation and\n",
    "latent space feature absraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3edc181-80d0-4b66-bcfe-e11314cff63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# Import necesary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print('Libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8cd18e-02ea-4172-9c9e-2eedda914b0b",
   "metadata": {},
   "source": [
    "# **1. GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "530d7861-a6f1-4ee6-95b6-d37fcd5b1212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Step [0/32], Generator Loss: 0.7762, Discriminator Loss: 1.4646\n",
      "Epoch [1/100], Step [0/32], Generator Loss: 0.6864, Discriminator Loss: 1.4245\n",
      "Epoch [2/100], Step [0/32], Generator Loss: 0.6427, Discriminator Loss: 1.4087\n",
      "Epoch [3/100], Step [0/32], Generator Loss: 0.6989, Discriminator Loss: 1.3179\n",
      "Epoch [4/100], Step [0/32], Generator Loss: 0.7660, Discriminator Loss: 1.2429\n",
      "Epoch [5/100], Step [0/32], Generator Loss: 0.7843, Discriminator Loss: 1.1737\n",
      "Epoch [6/100], Step [0/32], Generator Loss: 0.7729, Discriminator Loss: 1.1436\n",
      "Epoch [7/100], Step [0/32], Generator Loss: 0.7632, Discriminator Loss: 1.2006\n",
      "Epoch [8/100], Step [0/32], Generator Loss: 0.7291, Discriminator Loss: 1.1476\n",
      "Epoch [9/100], Step [0/32], Generator Loss: 0.6632, Discriminator Loss: 1.1909\n",
      "Epoch [10/100], Step [0/32], Generator Loss: 0.5584, Discriminator Loss: 1.3916\n",
      "Epoch [11/100], Step [0/32], Generator Loss: 0.5333, Discriminator Loss: 1.3338\n",
      "Epoch [12/100], Step [0/32], Generator Loss: 0.5769, Discriminator Loss: 1.3599\n",
      "Epoch [13/100], Step [0/32], Generator Loss: 0.6567, Discriminator Loss: 1.2903\n",
      "Epoch [14/100], Step [0/32], Generator Loss: 0.7241, Discriminator Loss: 1.1487\n",
      "Epoch [15/100], Step [0/32], Generator Loss: 0.7512, Discriminator Loss: 1.2430\n",
      "Epoch [16/100], Step [0/32], Generator Loss: 0.7339, Discriminator Loss: 1.2922\n",
      "Epoch [17/100], Step [0/32], Generator Loss: 0.7475, Discriminator Loss: 1.1880\n",
      "Epoch [18/100], Step [0/32], Generator Loss: 0.8109, Discriminator Loss: 1.2552\n",
      "Epoch [19/100], Step [0/32], Generator Loss: 0.9212, Discriminator Loss: 1.1741\n",
      "Epoch [20/100], Step [0/32], Generator Loss: 1.0287, Discriminator Loss: 1.2384\n",
      "Epoch [21/100], Step [0/32], Generator Loss: 0.9309, Discriminator Loss: 1.0635\n",
      "Epoch [22/100], Step [0/32], Generator Loss: 0.8670, Discriminator Loss: 1.2233\n",
      "Epoch [23/100], Step [0/32], Generator Loss: 0.7082, Discriminator Loss: 1.4715\n",
      "Epoch [24/100], Step [0/32], Generator Loss: 0.6123, Discriminator Loss: 1.4359\n",
      "Epoch [25/100], Step [0/32], Generator Loss: 0.6465, Discriminator Loss: 1.5680\n",
      "Epoch [26/100], Step [0/32], Generator Loss: 0.7687, Discriminator Loss: 1.4507\n",
      "Epoch [27/100], Step [0/32], Generator Loss: 0.9273, Discriminator Loss: 1.3964\n",
      "Epoch [28/100], Step [0/32], Generator Loss: 1.0786, Discriminator Loss: 1.2438\n",
      "Epoch [29/100], Step [0/32], Generator Loss: 1.0673, Discriminator Loss: 1.1457\n",
      "Epoch [30/100], Step [0/32], Generator Loss: 1.0901, Discriminator Loss: 1.1619\n",
      "Epoch [31/100], Step [0/32], Generator Loss: 1.0879, Discriminator Loss: 1.3024\n",
      "Epoch [32/100], Step [0/32], Generator Loss: 1.0540, Discriminator Loss: 1.2622\n",
      "Epoch [33/100], Step [0/32], Generator Loss: 0.9245, Discriminator Loss: 1.3321\n",
      "Epoch [34/100], Step [0/32], Generator Loss: 0.8011, Discriminator Loss: 1.4777\n",
      "Epoch [35/100], Step [0/32], Generator Loss: 0.6809, Discriminator Loss: 1.5086\n",
      "Epoch [36/100], Step [0/32], Generator Loss: 0.6574, Discriminator Loss: 1.6414\n",
      "Epoch [37/100], Step [0/32], Generator Loss: 0.6237, Discriminator Loss: 1.6006\n",
      "Epoch [38/100], Step [0/32], Generator Loss: 0.7314, Discriminator Loss: 1.4994\n",
      "Epoch [39/100], Step [0/32], Generator Loss: 0.7672, Discriminator Loss: 1.3424\n",
      "Epoch [40/100], Step [0/32], Generator Loss: 0.9800, Discriminator Loss: 1.2815\n",
      "Epoch [41/100], Step [0/32], Generator Loss: 0.9932, Discriminator Loss: 1.1960\n",
      "Epoch [42/100], Step [0/32], Generator Loss: 0.9527, Discriminator Loss: 1.2416\n",
      "Epoch [43/100], Step [0/32], Generator Loss: 0.8743, Discriminator Loss: 1.2215\n",
      "Epoch [44/100], Step [0/32], Generator Loss: 0.7502, Discriminator Loss: 1.2783\n",
      "Epoch [45/100], Step [0/32], Generator Loss: 0.7188, Discriminator Loss: 1.3281\n",
      "Epoch [46/100], Step [0/32], Generator Loss: 0.7120, Discriminator Loss: 1.3688\n",
      "Epoch [47/100], Step [0/32], Generator Loss: 0.7139, Discriminator Loss: 1.3264\n",
      "Epoch [48/100], Step [0/32], Generator Loss: 0.7820, Discriminator Loss: 1.3367\n",
      "Epoch [49/100], Step [0/32], Generator Loss: 0.7101, Discriminator Loss: 1.3090\n",
      "Epoch [50/100], Step [0/32], Generator Loss: 0.7837, Discriminator Loss: 1.4654\n",
      "Epoch [51/100], Step [0/32], Generator Loss: 0.7368, Discriminator Loss: 1.4336\n",
      "Epoch [52/100], Step [0/32], Generator Loss: 0.7362, Discriminator Loss: 1.3938\n",
      "Epoch [53/100], Step [0/32], Generator Loss: 0.7071, Discriminator Loss: 1.4688\n",
      "Epoch [54/100], Step [0/32], Generator Loss: 0.7295, Discriminator Loss: 1.5408\n",
      "Epoch [55/100], Step [0/32], Generator Loss: 0.8378, Discriminator Loss: 1.5000\n",
      "Epoch [56/100], Step [0/32], Generator Loss: 0.9072, Discriminator Loss: 1.3588\n",
      "Epoch [57/100], Step [0/32], Generator Loss: 1.0111, Discriminator Loss: 1.3436\n",
      "Epoch [58/100], Step [0/32], Generator Loss: 0.9384, Discriminator Loss: 1.3187\n",
      "Epoch [59/100], Step [0/32], Generator Loss: 1.0377, Discriminator Loss: 1.3705\n",
      "Epoch [60/100], Step [0/32], Generator Loss: 0.8143, Discriminator Loss: 1.5270\n",
      "Epoch [61/100], Step [0/32], Generator Loss: 0.7637, Discriminator Loss: 1.4610\n",
      "Epoch [62/100], Step [0/32], Generator Loss: 0.5852, Discriminator Loss: 1.6152\n",
      "Epoch [63/100], Step [0/32], Generator Loss: 0.6405, Discriminator Loss: 1.6142\n",
      "Epoch [64/100], Step [0/32], Generator Loss: 0.7085, Discriminator Loss: 1.4600\n",
      "Epoch [65/100], Step [0/32], Generator Loss: 0.7119, Discriminator Loss: 1.5094\n",
      "Epoch [66/100], Step [0/32], Generator Loss: 0.7440, Discriminator Loss: 1.4090\n",
      "Epoch [67/100], Step [0/32], Generator Loss: 0.7431, Discriminator Loss: 1.4502\n",
      "Epoch [68/100], Step [0/32], Generator Loss: 0.8077, Discriminator Loss: 1.4657\n",
      "Epoch [69/100], Step [0/32], Generator Loss: 0.7483, Discriminator Loss: 1.4976\n",
      "Epoch [70/100], Step [0/32], Generator Loss: 0.7730, Discriminator Loss: 1.4335\n",
      "Epoch [71/100], Step [0/32], Generator Loss: 0.6947, Discriminator Loss: 1.4336\n",
      "Epoch [72/100], Step [0/32], Generator Loss: 0.7558, Discriminator Loss: 1.5106\n",
      "Epoch [73/100], Step [0/32], Generator Loss: 0.6812, Discriminator Loss: 1.5378\n",
      "Epoch [74/100], Step [0/32], Generator Loss: 0.6982, Discriminator Loss: 1.5044\n",
      "Epoch [75/100], Step [0/32], Generator Loss: 0.6788, Discriminator Loss: 1.5011\n",
      "Epoch [76/100], Step [0/32], Generator Loss: 0.6995, Discriminator Loss: 1.4267\n",
      "Epoch [77/100], Step [0/32], Generator Loss: 0.7204, Discriminator Loss: 1.4770\n",
      "Epoch [78/100], Step [0/32], Generator Loss: 0.7861, Discriminator Loss: 1.3963\n",
      "Epoch [79/100], Step [0/32], Generator Loss: 0.7623, Discriminator Loss: 1.3138\n",
      "Epoch [80/100], Step [0/32], Generator Loss: 0.8047, Discriminator Loss: 1.3224\n",
      "Epoch [81/100], Step [0/32], Generator Loss: 0.8056, Discriminator Loss: 1.3903\n",
      "Epoch [82/100], Step [0/32], Generator Loss: 0.8423, Discriminator Loss: 1.3215\n",
      "Epoch [83/100], Step [0/32], Generator Loss: 0.8215, Discriminator Loss: 1.3804\n",
      "Epoch [84/100], Step [0/32], Generator Loss: 0.7395, Discriminator Loss: 1.3245\n",
      "Epoch [85/100], Step [0/32], Generator Loss: 0.6976, Discriminator Loss: 1.5044\n",
      "Epoch [86/100], Step [0/32], Generator Loss: 0.6637, Discriminator Loss: 1.5111\n",
      "Epoch [87/100], Step [0/32], Generator Loss: 0.6781, Discriminator Loss: 1.5033\n",
      "Epoch [88/100], Step [0/32], Generator Loss: 0.6733, Discriminator Loss: 1.5419\n",
      "Epoch [89/100], Step [0/32], Generator Loss: 0.7078, Discriminator Loss: 1.5395\n",
      "Epoch [90/100], Step [0/32], Generator Loss: 0.8421, Discriminator Loss: 1.3405\n",
      "Epoch [91/100], Step [0/32], Generator Loss: 0.8054, Discriminator Loss: 1.2996\n",
      "Epoch [92/100], Step [0/32], Generator Loss: 0.8791, Discriminator Loss: 1.2655\n",
      "Epoch [93/100], Step [0/32], Generator Loss: 0.8710, Discriminator Loss: 1.2729\n",
      "Epoch [94/100], Step [0/32], Generator Loss: 0.8129, Discriminator Loss: 1.3698\n",
      "Epoch [95/100], Step [0/32], Generator Loss: 0.7333, Discriminator Loss: 1.3628\n",
      "Epoch [96/100], Step [0/32], Generator Loss: 0.6878, Discriminator Loss: 1.4469\n",
      "Epoch [97/100], Step [0/32], Generator Loss: 0.6556, Discriminator Loss: 1.3929\n",
      "Epoch [98/100], Step [0/32], Generator Loss: 0.6873, Discriminator Loss: 1.4139\n",
      "Epoch [99/100], Step [0/32], Generator Loss: 0.7034, Discriminator Loss: 1.3417\n",
      "tensor([[-1.2869, -0.9539,  0.3900, -0.1768, -1.7610,  0.4412, -0.2953, -0.4103,\n",
      "          0.6826,  0.6774],\n",
      "        [-1.9132,  0.9892, -0.2097, -0.2738, -2.4131,  0.0860, -0.3753, -0.7008,\n",
      "          1.4210, -0.5127],\n",
      "        [-0.5984, -1.3914,  1.6216, -0.9123, -1.2181,  1.0796, -0.6605, -0.0515,\n",
      "          0.1206, -1.0604],\n",
      "        [-0.5747, -1.4120, -0.1037,  0.4284, -0.3030,  0.2595, -0.1251, -1.0238,\n",
      "          0.0689, -0.8385],\n",
      "        [-1.0881,  0.0289,  0.9925, -1.1444, -0.2556, -0.6022,  0.0609,  0.0166,\n",
      "          0.0059, -0.8305],\n",
      "        [ 0.2978,  0.7218,  2.5637, -0.8215,  0.3165, -0.7062,  0.1043,  0.2227,\n",
      "         -1.0643, -1.9792],\n",
      "        [-0.8225,  0.0214, -0.1670, -0.2946, -1.6385,  0.6364, -0.5127, -0.9254,\n",
      "          1.4946,  0.1213],\n",
      "        [ 0.4848, -0.1733,  1.7412,  0.1435,  0.3953, -0.4004, -0.2669, -0.0888,\n",
      "         -1.1585, -1.1953],\n",
      "        [-0.7810,  0.3431,  0.5651, -0.1311, -0.9184, -0.1869, -0.1785, -0.0356,\n",
      "          0.0767, -0.9916],\n",
      "        [-1.0383, -0.4415, -0.3224,  0.5062, -0.5291, -0.5185, -0.1930, -0.8652,\n",
      "         -0.0810, -0.5541]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a simple GAN for tabular data generation\n",
    "# Define Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "# Define Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Generate random tabular data for training\n",
    "def generate_random_data(num_samples, input_size):\n",
    "    return torch.randn(num_samples, input_size)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "input_size = 10  # Number of features in the tabular data\n",
    "latent_size = 20  # Size of the input noise vector for the generator\n",
    "data_size = 1000  # Number of samples in the dataset\n",
    "\n",
    "# Create generator and discriminator models\n",
    "generator = Generator(latent_size, input_size)\n",
    "discriminator = Discriminator(input_size)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Generate random tabular data\n",
    "real_data = generate_random_data(data_size, input_size)\n",
    "data_loader = DataLoader(TensorDataset(real_data), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(data_loader):\n",
    "        real_samples = data[0]\n",
    "        batch_size = real_samples.size(0)\n",
    "        real_labels = torch.ones(batch_size, 1)  # Labels for real data\n",
    "        fake_labels = torch.zeros(batch_size, 1)  # Labels for generated data\n",
    "        \n",
    "        # Train Discriminator \n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        # Train with real data\n",
    "        real_output = discriminator(real_samples)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "        \n",
    "        # Train with fake data\n",
    "        z = torch.randn(batch_size, latent_size)\n",
    "        fake_samples = generator(z)\n",
    "        fake_output = discriminator(fake_samples.detach())  # Detach to avoid backpropagating through G\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "        \n",
    "        # Combined loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train Generator \n",
    "        generator.zero_grad()\n",
    "        \n",
    "        # Generate fake data\n",
    "        z = torch.randn(batch_size, latent_size)\n",
    "        fake_samples = generator(z)\n",
    "        fake_output = discriminator(fake_samples)\n",
    "        \n",
    "        # Generator loss (maximize log(D(G(z))))\n",
    "        g_loss = criterion(fake_output, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(data_loader)}], \"\n",
    "                  f\"Generator Loss: {g_loss.item():.4f}, Discriminator Loss: {d_loss.item():.4f}\")\n",
    "# Generate augmented data\n",
    "augmented_examples = 10\n",
    "print(generator(torch.randn(augmented_examples, latent_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973e11b-cdda-4d40-8919-64631f7eb938",
   "metadata": {},
   "source": [
    "# **2. cGAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c050c28-69c3-49c4-b1e7-305d4455e9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Step [0/32], Generator Loss: 0.7127, Discriminator Loss: 1.3845\n",
      "Epoch [0/100], Step [10/32], Generator Loss: 0.7194, Discriminator Loss: 1.3795\n",
      "Epoch [0/100], Step [20/32], Generator Loss: 0.7173, Discriminator Loss: 1.3773\n",
      "Epoch [0/100], Step [30/32], Generator Loss: 0.7129, Discriminator Loss: 1.3743\n",
      "Epoch [1/100], Step [0/32], Generator Loss: 0.7101, Discriminator Loss: 1.3713\n",
      "Epoch [1/100], Step [10/32], Generator Loss: 0.7022, Discriminator Loss: 1.3742\n",
      "Epoch [1/100], Step [20/32], Generator Loss: 0.7012, Discriminator Loss: 1.3644\n",
      "Epoch [1/100], Step [30/32], Generator Loss: 0.6954, Discriminator Loss: 1.3704\n",
      "Epoch [2/100], Step [0/32], Generator Loss: 0.6985, Discriminator Loss: 1.3634\n",
      "Epoch [2/100], Step [10/32], Generator Loss: 0.6914, Discriminator Loss: 1.3665\n",
      "Epoch [2/100], Step [20/32], Generator Loss: 0.6890, Discriminator Loss: 1.3722\n",
      "Epoch [2/100], Step [30/32], Generator Loss: 0.6964, Discriminator Loss: 1.3729\n",
      "Epoch [3/100], Step [0/32], Generator Loss: 0.6944, Discriminator Loss: 1.3668\n",
      "Epoch [3/100], Step [10/32], Generator Loss: 0.7012, Discriminator Loss: 1.3565\n",
      "Epoch [3/100], Step [20/32], Generator Loss: 0.7048, Discriminator Loss: 1.3616\n",
      "Epoch [3/100], Step [30/32], Generator Loss: 0.7000, Discriminator Loss: 1.3623\n",
      "Epoch [4/100], Step [0/32], Generator Loss: 0.7005, Discriminator Loss: 1.3467\n",
      "Epoch [4/100], Step [10/32], Generator Loss: 0.7061, Discriminator Loss: 1.3570\n",
      "Epoch [4/100], Step [20/32], Generator Loss: 0.6978, Discriminator Loss: 1.3543\n",
      "Epoch [4/100], Step [30/32], Generator Loss: 0.6978, Discriminator Loss: 1.3450\n",
      "Epoch [5/100], Step [0/32], Generator Loss: 0.7016, Discriminator Loss: 1.3515\n",
      "Epoch [5/100], Step [10/32], Generator Loss: 0.7143, Discriminator Loss: 1.3401\n",
      "Epoch [5/100], Step [20/32], Generator Loss: 0.7179, Discriminator Loss: 1.3417\n",
      "Epoch [5/100], Step [30/32], Generator Loss: 0.7299, Discriminator Loss: 1.3430\n",
      "Epoch [6/100], Step [0/32], Generator Loss: 0.7350, Discriminator Loss: 1.3351\n",
      "Epoch [6/100], Step [10/32], Generator Loss: 0.7311, Discriminator Loss: 1.3358\n",
      "Epoch [6/100], Step [20/32], Generator Loss: 0.7257, Discriminator Loss: 1.3402\n",
      "Epoch [6/100], Step [30/32], Generator Loss: 0.7125, Discriminator Loss: 1.3476\n",
      "Epoch [7/100], Step [0/32], Generator Loss: 0.7093, Discriminator Loss: 1.3424\n",
      "Epoch [7/100], Step [10/32], Generator Loss: 0.7117, Discriminator Loss: 1.3309\n",
      "Epoch [7/100], Step [20/32], Generator Loss: 0.7064, Discriminator Loss: 1.3406\n",
      "Epoch [7/100], Step [30/32], Generator Loss: 0.6955, Discriminator Loss: 1.3492\n",
      "Epoch [8/100], Step [0/32], Generator Loss: 0.6948, Discriminator Loss: 1.3527\n",
      "Epoch [8/100], Step [10/32], Generator Loss: 0.6834, Discriminator Loss: 1.3415\n",
      "Epoch [8/100], Step [20/32], Generator Loss: 0.6815, Discriminator Loss: 1.3511\n",
      "Epoch [8/100], Step [30/32], Generator Loss: 0.6787, Discriminator Loss: 1.3332\n",
      "Epoch [9/100], Step [0/32], Generator Loss: 0.6832, Discriminator Loss: 1.3511\n",
      "Epoch [9/100], Step [10/32], Generator Loss: 0.7006, Discriminator Loss: 1.3446\n",
      "Epoch [9/100], Step [20/32], Generator Loss: 0.7092, Discriminator Loss: 1.3300\n",
      "Epoch [9/100], Step [30/32], Generator Loss: 0.7162, Discriminator Loss: 1.3459\n",
      "Epoch [10/100], Step [0/32], Generator Loss: 0.7112, Discriminator Loss: 1.3372\n",
      "Epoch [10/100], Step [10/32], Generator Loss: 0.7115, Discriminator Loss: 1.3310\n",
      "Epoch [10/100], Step [20/32], Generator Loss: 0.7261, Discriminator Loss: 1.3214\n",
      "Epoch [10/100], Step [30/32], Generator Loss: 0.7409, Discriminator Loss: 1.3207\n",
      "Epoch [11/100], Step [0/32], Generator Loss: 0.7310, Discriminator Loss: 1.3357\n",
      "Epoch [11/100], Step [10/32], Generator Loss: 0.7147, Discriminator Loss: 1.3354\n",
      "Epoch [11/100], Step [20/32], Generator Loss: 0.6917, Discriminator Loss: 1.3375\n",
      "Epoch [11/100], Step [30/32], Generator Loss: 0.6941, Discriminator Loss: 1.3318\n",
      "Epoch [12/100], Step [0/32], Generator Loss: 0.6960, Discriminator Loss: 1.3437\n",
      "Epoch [12/100], Step [10/32], Generator Loss: 0.6933, Discriminator Loss: 1.3513\n",
      "Epoch [12/100], Step [20/32], Generator Loss: 0.6960, Discriminator Loss: 1.3305\n",
      "Epoch [12/100], Step [30/32], Generator Loss: 0.6749, Discriminator Loss: 1.3505\n",
      "Epoch [13/100], Step [0/32], Generator Loss: 0.6764, Discriminator Loss: 1.3339\n",
      "Epoch [13/100], Step [10/32], Generator Loss: 0.6793, Discriminator Loss: 1.3086\n",
      "Epoch [13/100], Step [20/32], Generator Loss: 0.6993, Discriminator Loss: 1.3165\n",
      "Epoch [13/100], Step [30/32], Generator Loss: 0.7073, Discriminator Loss: 1.3361\n",
      "Epoch [14/100], Step [0/32], Generator Loss: 0.7082, Discriminator Loss: 1.3129\n",
      "Epoch [14/100], Step [10/32], Generator Loss: 0.6997, Discriminator Loss: 1.3035\n",
      "Epoch [14/100], Step [20/32], Generator Loss: 0.7166, Discriminator Loss: 1.3398\n",
      "Epoch [14/100], Step [30/32], Generator Loss: 0.7075, Discriminator Loss: 1.3091\n",
      "Epoch [15/100], Step [0/32], Generator Loss: 0.7119, Discriminator Loss: 1.3111\n",
      "Epoch [15/100], Step [10/32], Generator Loss: 0.6796, Discriminator Loss: 1.3530\n",
      "Epoch [15/100], Step [20/32], Generator Loss: 0.6688, Discriminator Loss: 1.3416\n",
      "Epoch [15/100], Step [30/32], Generator Loss: 0.6987, Discriminator Loss: 1.3450\n",
      "Epoch [16/100], Step [0/32], Generator Loss: 0.7017, Discriminator Loss: 1.3637\n",
      "Epoch [16/100], Step [10/32], Generator Loss: 0.7038, Discriminator Loss: 1.3414\n",
      "Epoch [16/100], Step [20/32], Generator Loss: 0.7239, Discriminator Loss: 1.3484\n",
      "Epoch [16/100], Step [30/32], Generator Loss: 0.7488, Discriminator Loss: 1.3546\n",
      "Epoch [17/100], Step [0/32], Generator Loss: 0.7511, Discriminator Loss: 1.3648\n",
      "Epoch [17/100], Step [10/32], Generator Loss: 0.7737, Discriminator Loss: 1.3475\n",
      "Epoch [17/100], Step [20/32], Generator Loss: 0.7853, Discriminator Loss: 1.3382\n",
      "Epoch [17/100], Step [30/32], Generator Loss: 0.7968, Discriminator Loss: 1.3088\n",
      "Epoch [18/100], Step [0/32], Generator Loss: 0.8016, Discriminator Loss: 1.3308\n",
      "Epoch [18/100], Step [10/32], Generator Loss: 0.8129, Discriminator Loss: 1.3005\n",
      "Epoch [18/100], Step [20/32], Generator Loss: 0.8182, Discriminator Loss: 1.2963\n",
      "Epoch [18/100], Step [30/32], Generator Loss: 0.7757, Discriminator Loss: 1.2815\n",
      "Epoch [19/100], Step [0/32], Generator Loss: 0.7869, Discriminator Loss: 1.2783\n",
      "Epoch [19/100], Step [10/32], Generator Loss: 0.7512, Discriminator Loss: 1.2720\n",
      "Epoch [19/100], Step [20/32], Generator Loss: 0.7575, Discriminator Loss: 1.2692\n",
      "Epoch [19/100], Step [30/32], Generator Loss: 0.7463, Discriminator Loss: 1.2834\n",
      "Epoch [20/100], Step [0/32], Generator Loss: 0.7387, Discriminator Loss: 1.2896\n",
      "Epoch [20/100], Step [10/32], Generator Loss: 0.7439, Discriminator Loss: 1.2973\n",
      "Epoch [20/100], Step [20/32], Generator Loss: 0.7373, Discriminator Loss: 1.3074\n",
      "Epoch [20/100], Step [30/32], Generator Loss: 0.7291, Discriminator Loss: 1.3224\n",
      "Epoch [21/100], Step [0/32], Generator Loss: 0.7294, Discriminator Loss: 1.3489\n",
      "Epoch [21/100], Step [10/32], Generator Loss: 0.7156, Discriminator Loss: 1.3016\n",
      "Epoch [21/100], Step [20/32], Generator Loss: 0.7217, Discriminator Loss: 1.3764\n",
      "Epoch [21/100], Step [30/32], Generator Loss: 0.6891, Discriminator Loss: 1.3613\n",
      "Epoch [22/100], Step [0/32], Generator Loss: 0.6823, Discriminator Loss: 1.3448\n",
      "Epoch [22/100], Step [10/32], Generator Loss: 0.6567, Discriminator Loss: 1.4021\n",
      "Epoch [22/100], Step [20/32], Generator Loss: 0.6345, Discriminator Loss: 1.3865\n",
      "Epoch [22/100], Step [30/32], Generator Loss: 0.6527, Discriminator Loss: 1.3680\n",
      "Epoch [23/100], Step [0/32], Generator Loss: 0.6553, Discriminator Loss: 1.3571\n",
      "Epoch [23/100], Step [10/32], Generator Loss: 0.6546, Discriminator Loss: 1.3236\n",
      "Epoch [23/100], Step [20/32], Generator Loss: 0.6431, Discriminator Loss: 1.3841\n",
      "Epoch [23/100], Step [30/32], Generator Loss: 0.6339, Discriminator Loss: 1.3847\n",
      "Epoch [24/100], Step [0/32], Generator Loss: 0.6318, Discriminator Loss: 1.3587\n",
      "Epoch [24/100], Step [10/32], Generator Loss: 0.6272, Discriminator Loss: 1.3546\n",
      "Epoch [24/100], Step [20/32], Generator Loss: 0.5876, Discriminator Loss: 1.3743\n",
      "Epoch [24/100], Step [30/32], Generator Loss: 0.6028, Discriminator Loss: 1.3942\n",
      "Epoch [25/100], Step [0/32], Generator Loss: 0.6067, Discriminator Loss: 1.3608\n",
      "Epoch [25/100], Step [10/32], Generator Loss: 0.6251, Discriminator Loss: 1.3503\n",
      "Epoch [25/100], Step [20/32], Generator Loss: 0.6657, Discriminator Loss: 1.3749\n",
      "Epoch [25/100], Step [30/32], Generator Loss: 0.7038, Discriminator Loss: 1.3175\n",
      "Epoch [26/100], Step [0/32], Generator Loss: 0.7074, Discriminator Loss: 1.3304\n",
      "Epoch [26/100], Step [10/32], Generator Loss: 0.7238, Discriminator Loss: 1.3050\n",
      "Epoch [26/100], Step [20/32], Generator Loss: 0.7755, Discriminator Loss: 1.2725\n",
      "Epoch [26/100], Step [30/32], Generator Loss: 0.8008, Discriminator Loss: 1.2145\n",
      "Epoch [27/100], Step [0/32], Generator Loss: 0.8059, Discriminator Loss: 1.2566\n",
      "Epoch [27/100], Step [10/32], Generator Loss: 0.8299, Discriminator Loss: 1.1903\n",
      "Epoch [27/100], Step [20/32], Generator Loss: 0.8441, Discriminator Loss: 1.1698\n",
      "Epoch [27/100], Step [30/32], Generator Loss: 0.8608, Discriminator Loss: 1.1736\n",
      "Epoch [28/100], Step [0/32], Generator Loss: 0.8706, Discriminator Loss: 1.1745\n",
      "Epoch [28/100], Step [10/32], Generator Loss: 0.8435, Discriminator Loss: 1.2026\n",
      "Epoch [28/100], Step [20/32], Generator Loss: 0.8038, Discriminator Loss: 1.1857\n",
      "Epoch [28/100], Step [30/32], Generator Loss: 0.7887, Discriminator Loss: 1.1947\n",
      "Epoch [29/100], Step [0/32], Generator Loss: 0.7629, Discriminator Loss: 1.2056\n",
      "Epoch [29/100], Step [10/32], Generator Loss: 0.7504, Discriminator Loss: 1.2342\n",
      "Epoch [29/100], Step [20/32], Generator Loss: 0.7664, Discriminator Loss: 1.1923\n",
      "Epoch [29/100], Step [30/32], Generator Loss: 0.7288, Discriminator Loss: 1.2300\n",
      "Epoch [30/100], Step [0/32], Generator Loss: 0.7552, Discriminator Loss: 1.2227\n",
      "Epoch [30/100], Step [10/32], Generator Loss: 0.6997, Discriminator Loss: 1.2316\n",
      "Epoch [30/100], Step [20/32], Generator Loss: 0.6456, Discriminator Loss: 1.2866\n",
      "Epoch [30/100], Step [30/32], Generator Loss: 0.6485, Discriminator Loss: 1.3685\n",
      "Epoch [31/100], Step [0/32], Generator Loss: 0.6489, Discriminator Loss: 1.3904\n",
      "Epoch [31/100], Step [10/32], Generator Loss: 0.6452, Discriminator Loss: 1.3736\n",
      "Epoch [31/100], Step [20/32], Generator Loss: 0.6660, Discriminator Loss: 1.3560\n",
      "Epoch [31/100], Step [30/32], Generator Loss: 0.6765, Discriminator Loss: 1.4380\n",
      "Epoch [32/100], Step [0/32], Generator Loss: 0.6801, Discriminator Loss: 1.3601\n",
      "Epoch [32/100], Step [10/32], Generator Loss: 0.6875, Discriminator Loss: 1.4396\n",
      "Epoch [32/100], Step [20/32], Generator Loss: 0.6756, Discriminator Loss: 1.3907\n",
      "Epoch [32/100], Step [30/32], Generator Loss: 0.6370, Discriminator Loss: 1.4400\n",
      "Epoch [33/100], Step [0/32], Generator Loss: 0.6352, Discriminator Loss: 1.4356\n",
      "Epoch [33/100], Step [10/32], Generator Loss: 0.6405, Discriminator Loss: 1.4347\n",
      "Epoch [33/100], Step [20/32], Generator Loss: 0.6635, Discriminator Loss: 1.4571\n",
      "Epoch [33/100], Step [30/32], Generator Loss: 0.6537, Discriminator Loss: 1.4291\n",
      "Epoch [34/100], Step [0/32], Generator Loss: 0.6406, Discriminator Loss: 1.4495\n",
      "Epoch [34/100], Step [10/32], Generator Loss: 0.6418, Discriminator Loss: 1.4022\n",
      "Epoch [34/100], Step [20/32], Generator Loss: 0.6687, Discriminator Loss: 1.4020\n",
      "Epoch [34/100], Step [30/32], Generator Loss: 0.6804, Discriminator Loss: 1.3881\n",
      "Epoch [35/100], Step [0/32], Generator Loss: 0.6600, Discriminator Loss: 1.3486\n",
      "Epoch [35/100], Step [10/32], Generator Loss: 0.6815, Discriminator Loss: 1.3869\n",
      "Epoch [35/100], Step [20/32], Generator Loss: 0.7153, Discriminator Loss: 1.3443\n",
      "Epoch [35/100], Step [30/32], Generator Loss: 0.7131, Discriminator Loss: 1.3270\n",
      "Epoch [36/100], Step [0/32], Generator Loss: 0.7129, Discriminator Loss: 1.3384\n",
      "Epoch [36/100], Step [10/32], Generator Loss: 0.7023, Discriminator Loss: 1.3215\n",
      "Epoch [36/100], Step [20/32], Generator Loss: 0.6957, Discriminator Loss: 1.2698\n",
      "Epoch [36/100], Step [30/32], Generator Loss: 0.7083, Discriminator Loss: 1.2653\n",
      "Epoch [37/100], Step [0/32], Generator Loss: 0.7168, Discriminator Loss: 1.2635\n",
      "Epoch [37/100], Step [10/32], Generator Loss: 0.7189, Discriminator Loss: 1.2651\n",
      "Epoch [37/100], Step [20/32], Generator Loss: 0.7151, Discriminator Loss: 1.2353\n",
      "Epoch [37/100], Step [30/32], Generator Loss: 0.7609, Discriminator Loss: 1.2203\n",
      "Epoch [38/100], Step [0/32], Generator Loss: 0.7717, Discriminator Loss: 1.1949\n",
      "Epoch [38/100], Step [10/32], Generator Loss: 0.7830, Discriminator Loss: 1.1903\n",
      "Epoch [38/100], Step [20/32], Generator Loss: 0.7884, Discriminator Loss: 1.1909\n",
      "Epoch [38/100], Step [30/32], Generator Loss: 0.8188, Discriminator Loss: 1.1488\n",
      "Epoch [39/100], Step [0/32], Generator Loss: 0.8229, Discriminator Loss: 1.0998\n",
      "Epoch [39/100], Step [10/32], Generator Loss: 0.8334, Discriminator Loss: 1.1238\n",
      "Epoch [39/100], Step [20/32], Generator Loss: 0.8365, Discriminator Loss: 1.1748\n",
      "Epoch [39/100], Step [30/32], Generator Loss: 0.8444, Discriminator Loss: 1.1107\n",
      "Epoch [40/100], Step [0/32], Generator Loss: 0.8459, Discriminator Loss: 1.0956\n",
      "Epoch [40/100], Step [10/32], Generator Loss: 0.8393, Discriminator Loss: 1.1082\n",
      "Epoch [40/100], Step [20/32], Generator Loss: 0.8236, Discriminator Loss: 1.0599\n",
      "Epoch [40/100], Step [30/32], Generator Loss: 0.8220, Discriminator Loss: 1.0834\n",
      "Epoch [41/100], Step [0/32], Generator Loss: 0.8244, Discriminator Loss: 1.1542\n",
      "Epoch [41/100], Step [10/32], Generator Loss: 0.7593, Discriminator Loss: 1.1628\n",
      "Epoch [41/100], Step [20/32], Generator Loss: 0.7220, Discriminator Loss: 1.1585\n",
      "Epoch [41/100], Step [30/32], Generator Loss: 0.7352, Discriminator Loss: 1.2148\n",
      "Epoch [42/100], Step [0/32], Generator Loss: 0.7135, Discriminator Loss: 1.2022\n",
      "Epoch [42/100], Step [10/32], Generator Loss: 0.7067, Discriminator Loss: 1.1981\n",
      "Epoch [42/100], Step [20/32], Generator Loss: 0.7181, Discriminator Loss: 1.2257\n",
      "Epoch [42/100], Step [30/32], Generator Loss: 0.7106, Discriminator Loss: 1.2909\n",
      "Epoch [43/100], Step [0/32], Generator Loss: 0.6972, Discriminator Loss: 1.2276\n",
      "Epoch [43/100], Step [10/32], Generator Loss: 0.6935, Discriminator Loss: 1.3099\n",
      "Epoch [43/100], Step [20/32], Generator Loss: 0.6370, Discriminator Loss: 1.2995\n",
      "Epoch [43/100], Step [30/32], Generator Loss: 0.6143, Discriminator Loss: 1.3519\n",
      "Epoch [44/100], Step [0/32], Generator Loss: 0.6332, Discriminator Loss: 1.2913\n",
      "Epoch [44/100], Step [10/32], Generator Loss: 0.6132, Discriminator Loss: 1.3616\n",
      "Epoch [44/100], Step [20/32], Generator Loss: 0.5810, Discriminator Loss: 1.4229\n",
      "Epoch [44/100], Step [30/32], Generator Loss: 0.6005, Discriminator Loss: 1.4026\n",
      "Epoch [45/100], Step [0/32], Generator Loss: 0.5712, Discriminator Loss: 1.3627\n",
      "Epoch [45/100], Step [10/32], Generator Loss: 0.5611, Discriminator Loss: 1.4043\n",
      "Epoch [45/100], Step [20/32], Generator Loss: 0.5342, Discriminator Loss: 1.4436\n",
      "Epoch [45/100], Step [30/32], Generator Loss: 0.5462, Discriminator Loss: 1.4634\n",
      "Epoch [46/100], Step [0/32], Generator Loss: 0.5791, Discriminator Loss: 1.4574\n",
      "Epoch [46/100], Step [10/32], Generator Loss: 0.5977, Discriminator Loss: 1.4584\n",
      "Epoch [46/100], Step [20/32], Generator Loss: 0.6353, Discriminator Loss: 1.4478\n",
      "Epoch [46/100], Step [30/32], Generator Loss: 0.6773, Discriminator Loss: 1.3930\n",
      "Epoch [47/100], Step [0/32], Generator Loss: 0.6792, Discriminator Loss: 1.3581\n",
      "Epoch [47/100], Step [10/32], Generator Loss: 0.6801, Discriminator Loss: 1.4115\n",
      "Epoch [47/100], Step [20/32], Generator Loss: 0.7232, Discriminator Loss: 1.3734\n",
      "Epoch [47/100], Step [30/32], Generator Loss: 0.7763, Discriminator Loss: 1.3405\n",
      "Epoch [48/100], Step [0/32], Generator Loss: 0.7942, Discriminator Loss: 1.3137\n",
      "Epoch [48/100], Step [10/32], Generator Loss: 0.7813, Discriminator Loss: 1.2607\n",
      "Epoch [48/100], Step [20/32], Generator Loss: 0.7972, Discriminator Loss: 1.2715\n",
      "Epoch [48/100], Step [30/32], Generator Loss: 0.8149, Discriminator Loss: 1.2900\n",
      "Epoch [49/100], Step [0/32], Generator Loss: 0.8069, Discriminator Loss: 1.2446\n",
      "Epoch [49/100], Step [10/32], Generator Loss: 0.7827, Discriminator Loss: 1.2386\n",
      "Epoch [49/100], Step [20/32], Generator Loss: 0.6976, Discriminator Loss: 1.3049\n",
      "Epoch [49/100], Step [30/32], Generator Loss: 0.6662, Discriminator Loss: 1.3098\n",
      "Epoch [50/100], Step [0/32], Generator Loss: 0.6628, Discriminator Loss: 1.3389\n",
      "Epoch [50/100], Step [10/32], Generator Loss: 0.6311, Discriminator Loss: 1.3121\n",
      "Epoch [50/100], Step [20/32], Generator Loss: 0.6549, Discriminator Loss: 1.3630\n",
      "Epoch [50/100], Step [30/32], Generator Loss: 0.6847, Discriminator Loss: 1.3463\n",
      "Epoch [51/100], Step [0/32], Generator Loss: 0.6985, Discriminator Loss: 1.4183\n",
      "Epoch [51/100], Step [10/32], Generator Loss: 0.7178, Discriminator Loss: 1.3150\n",
      "Epoch [51/100], Step [20/32], Generator Loss: 0.7276, Discriminator Loss: 1.3110\n",
      "Epoch [51/100], Step [30/32], Generator Loss: 0.7506, Discriminator Loss: 1.2984\n",
      "Epoch [52/100], Step [0/32], Generator Loss: 0.7628, Discriminator Loss: 1.2748\n",
      "Epoch [52/100], Step [10/32], Generator Loss: 0.7949, Discriminator Loss: 1.3210\n",
      "Epoch [52/100], Step [20/32], Generator Loss: 0.8015, Discriminator Loss: 1.2432\n",
      "Epoch [52/100], Step [30/32], Generator Loss: 0.7794, Discriminator Loss: 1.2140\n",
      "Epoch [53/100], Step [0/32], Generator Loss: 0.7792, Discriminator Loss: 1.2082\n",
      "Epoch [53/100], Step [10/32], Generator Loss: 0.7775, Discriminator Loss: 1.2441\n",
      "Epoch [53/100], Step [20/32], Generator Loss: 0.7874, Discriminator Loss: 1.2503\n",
      "Epoch [53/100], Step [30/32], Generator Loss: 0.8050, Discriminator Loss: 1.1881\n",
      "Epoch [54/100], Step [0/32], Generator Loss: 0.8084, Discriminator Loss: 1.1839\n",
      "Epoch [54/100], Step [10/32], Generator Loss: 0.8236, Discriminator Loss: 1.2051\n",
      "Epoch [54/100], Step [20/32], Generator Loss: 0.8435, Discriminator Loss: 1.1708\n",
      "Epoch [54/100], Step [30/32], Generator Loss: 0.8560, Discriminator Loss: 1.1192\n",
      "Epoch [55/100], Step [0/32], Generator Loss: 0.8536, Discriminator Loss: 1.1442\n",
      "Epoch [55/100], Step [10/32], Generator Loss: 0.8632, Discriminator Loss: 1.1432\n",
      "Epoch [55/100], Step [20/32], Generator Loss: 0.8601, Discriminator Loss: 1.1264\n",
      "Epoch [55/100], Step [30/32], Generator Loss: 0.8668, Discriminator Loss: 1.0980\n",
      "Epoch [56/100], Step [0/32], Generator Loss: 0.8785, Discriminator Loss: 1.0709\n",
      "Epoch [56/100], Step [10/32], Generator Loss: 0.8689, Discriminator Loss: 1.0754\n",
      "Epoch [56/100], Step [20/32], Generator Loss: 0.8368, Discriminator Loss: 1.0711\n",
      "Epoch [56/100], Step [30/32], Generator Loss: 0.8474, Discriminator Loss: 1.0889\n",
      "Epoch [57/100], Step [0/32], Generator Loss: 0.8457, Discriminator Loss: 1.0887\n",
      "Epoch [57/100], Step [10/32], Generator Loss: 0.8430, Discriminator Loss: 1.0381\n",
      "Epoch [57/100], Step [20/32], Generator Loss: 0.8696, Discriminator Loss: 1.0996\n",
      "Epoch [57/100], Step [30/32], Generator Loss: 0.8659, Discriminator Loss: 1.0339\n",
      "Epoch [58/100], Step [0/32], Generator Loss: 0.8631, Discriminator Loss: 1.0492\n",
      "Epoch [58/100], Step [10/32], Generator Loss: 0.8736, Discriminator Loss: 1.0621\n",
      "Epoch [58/100], Step [20/32], Generator Loss: 0.8918, Discriminator Loss: 1.0257\n",
      "Epoch [58/100], Step [30/32], Generator Loss: 0.8957, Discriminator Loss: 0.9899\n",
      "Epoch [59/100], Step [0/32], Generator Loss: 0.8912, Discriminator Loss: 0.9942\n",
      "Epoch [59/100], Step [10/32], Generator Loss: 0.8039, Discriminator Loss: 1.0822\n",
      "Epoch [59/100], Step [20/32], Generator Loss: 0.7345, Discriminator Loss: 1.1654\n",
      "Epoch [59/100], Step [30/32], Generator Loss: 0.6559, Discriminator Loss: 1.1441\n",
      "Epoch [60/100], Step [0/32], Generator Loss: 0.6664, Discriminator Loss: 1.1658\n",
      "Epoch [60/100], Step [10/32], Generator Loss: 0.5793, Discriminator Loss: 1.2189\n",
      "Epoch [60/100], Step [20/32], Generator Loss: 0.6189, Discriminator Loss: 1.1903\n",
      "Epoch [60/100], Step [30/32], Generator Loss: 0.6239, Discriminator Loss: 1.4382\n",
      "Epoch [61/100], Step [0/32], Generator Loss: 0.5893, Discriminator Loss: 1.3756\n",
      "Epoch [61/100], Step [10/32], Generator Loss: 0.6166, Discriminator Loss: 1.3593\n",
      "Epoch [61/100], Step [20/32], Generator Loss: 0.5855, Discriminator Loss: 1.3635\n",
      "Epoch [61/100], Step [30/32], Generator Loss: 0.5930, Discriminator Loss: 1.5172\n",
      "Epoch [62/100], Step [0/32], Generator Loss: 0.5923, Discriminator Loss: 1.3811\n",
      "Epoch [62/100], Step [10/32], Generator Loss: 0.5764, Discriminator Loss: 1.4091\n",
      "Epoch [62/100], Step [20/32], Generator Loss: 0.5647, Discriminator Loss: 1.3931\n",
      "Epoch [62/100], Step [30/32], Generator Loss: 0.6091, Discriminator Loss: 1.3938\n",
      "Epoch [63/100], Step [0/32], Generator Loss: 0.6125, Discriminator Loss: 1.2598\n",
      "Epoch [63/100], Step [10/32], Generator Loss: 0.6035, Discriminator Loss: 1.4697\n",
      "Epoch [63/100], Step [20/32], Generator Loss: 0.6138, Discriminator Loss: 1.3408\n",
      "Epoch [63/100], Step [30/32], Generator Loss: 0.6403, Discriminator Loss: 1.3197\n",
      "Epoch [64/100], Step [0/32], Generator Loss: 0.6513, Discriminator Loss: 1.2549\n",
      "Epoch [64/100], Step [10/32], Generator Loss: 0.6796, Discriminator Loss: 1.3514\n",
      "Epoch [64/100], Step [20/32], Generator Loss: 0.7076, Discriminator Loss: 1.3659\n",
      "Epoch [64/100], Step [30/32], Generator Loss: 0.7317, Discriminator Loss: 1.1836\n",
      "Epoch [65/100], Step [0/32], Generator Loss: 0.7345, Discriminator Loss: 1.3338\n",
      "Epoch [65/100], Step [10/32], Generator Loss: 0.7371, Discriminator Loss: 1.2441\n",
      "Epoch [65/100], Step [20/32], Generator Loss: 0.7162, Discriminator Loss: 1.2669\n",
      "Epoch [65/100], Step [30/32], Generator Loss: 0.7244, Discriminator Loss: 1.2130\n",
      "Epoch [66/100], Step [0/32], Generator Loss: 0.7456, Discriminator Loss: 1.1957\n",
      "Epoch [66/100], Step [10/32], Generator Loss: 0.7544, Discriminator Loss: 1.2239\n",
      "Epoch [66/100], Step [20/32], Generator Loss: 0.7750, Discriminator Loss: 1.2281\n",
      "Epoch [66/100], Step [30/32], Generator Loss: 0.7672, Discriminator Loss: 1.2053\n",
      "Epoch [67/100], Step [0/32], Generator Loss: 0.7639, Discriminator Loss: 1.2026\n",
      "Epoch [67/100], Step [10/32], Generator Loss: 0.7808, Discriminator Loss: 1.1824\n",
      "Epoch [67/100], Step [20/32], Generator Loss: 0.7993, Discriminator Loss: 1.2258\n",
      "Epoch [67/100], Step [30/32], Generator Loss: 0.8063, Discriminator Loss: 1.1698\n",
      "Epoch [68/100], Step [0/32], Generator Loss: 0.8028, Discriminator Loss: 1.2020\n",
      "Epoch [68/100], Step [10/32], Generator Loss: 0.7859, Discriminator Loss: 1.2017\n",
      "Epoch [68/100], Step [20/32], Generator Loss: 0.7930, Discriminator Loss: 1.1800\n",
      "Epoch [68/100], Step [30/32], Generator Loss: 0.7706, Discriminator Loss: 1.1113\n",
      "Epoch [69/100], Step [0/32], Generator Loss: 0.7769, Discriminator Loss: 1.1375\n",
      "Epoch [69/100], Step [10/32], Generator Loss: 0.7804, Discriminator Loss: 1.1564\n",
      "Epoch [69/100], Step [20/32], Generator Loss: 0.7415, Discriminator Loss: 1.1920\n",
      "Epoch [69/100], Step [30/32], Generator Loss: 0.7165, Discriminator Loss: 1.2065\n",
      "Epoch [70/100], Step [0/32], Generator Loss: 0.7168, Discriminator Loss: 1.2137\n",
      "Epoch [70/100], Step [10/32], Generator Loss: 0.6992, Discriminator Loss: 1.2737\n",
      "Epoch [70/100], Step [20/32], Generator Loss: 0.6971, Discriminator Loss: 1.3068\n",
      "Epoch [70/100], Step [30/32], Generator Loss: 0.6453, Discriminator Loss: 1.3077\n",
      "Epoch [71/100], Step [0/32], Generator Loss: 0.6386, Discriminator Loss: 1.2744\n",
      "Epoch [71/100], Step [10/32], Generator Loss: 0.6472, Discriminator Loss: 1.3048\n",
      "Epoch [71/100], Step [20/32], Generator Loss: 0.6213, Discriminator Loss: 1.3423\n",
      "Epoch [71/100], Step [30/32], Generator Loss: 0.5926, Discriminator Loss: 1.2750\n",
      "Epoch [72/100], Step [0/32], Generator Loss: 0.6000, Discriminator Loss: 1.3060\n",
      "Epoch [72/100], Step [10/32], Generator Loss: 0.6161, Discriminator Loss: 1.2858\n",
      "Epoch [72/100], Step [20/32], Generator Loss: 0.6280, Discriminator Loss: 1.2875\n",
      "Epoch [72/100], Step [30/32], Generator Loss: 0.6426, Discriminator Loss: 1.2622\n",
      "Epoch [73/100], Step [0/32], Generator Loss: 0.6363, Discriminator Loss: 1.2681\n",
      "Epoch [73/100], Step [10/32], Generator Loss: 0.6724, Discriminator Loss: 1.2993\n",
      "Epoch [73/100], Step [20/32], Generator Loss: 0.6699, Discriminator Loss: 1.2362\n",
      "Epoch [73/100], Step [30/32], Generator Loss: 0.6845, Discriminator Loss: 1.2446\n",
      "Epoch [74/100], Step [0/32], Generator Loss: 0.6960, Discriminator Loss: 1.2091\n",
      "Epoch [74/100], Step [10/32], Generator Loss: 0.7077, Discriminator Loss: 1.2431\n",
      "Epoch [74/100], Step [20/32], Generator Loss: 0.7119, Discriminator Loss: 1.2729\n",
      "Epoch [74/100], Step [30/32], Generator Loss: 0.7254, Discriminator Loss: 1.1528\n",
      "Epoch [75/100], Step [0/32], Generator Loss: 0.7242, Discriminator Loss: 1.2186\n",
      "Epoch [75/100], Step [10/32], Generator Loss: 0.7378, Discriminator Loss: 1.1877\n",
      "Epoch [75/100], Step [20/32], Generator Loss: 0.7295, Discriminator Loss: 1.0917\n",
      "Epoch [75/100], Step [30/32], Generator Loss: 0.7036, Discriminator Loss: 1.2006\n",
      "Epoch [76/100], Step [0/32], Generator Loss: 0.7024, Discriminator Loss: 1.1511\n",
      "Epoch [76/100], Step [10/32], Generator Loss: 0.7222, Discriminator Loss: 1.1438\n",
      "Epoch [76/100], Step [20/32], Generator Loss: 0.7598, Discriminator Loss: 1.2318\n",
      "Epoch [76/100], Step [30/32], Generator Loss: 0.7358, Discriminator Loss: 1.2642\n",
      "Epoch [77/100], Step [0/32], Generator Loss: 0.7403, Discriminator Loss: 1.2733\n",
      "Epoch [77/100], Step [10/32], Generator Loss: 0.7273, Discriminator Loss: 1.2233\n",
      "Epoch [77/100], Step [20/32], Generator Loss: 0.7293, Discriminator Loss: 1.2799\n",
      "Epoch [77/100], Step [30/32], Generator Loss: 0.7444, Discriminator Loss: 1.2996\n",
      "Epoch [78/100], Step [0/32], Generator Loss: 0.7321, Discriminator Loss: 1.2161\n",
      "Epoch [78/100], Step [10/32], Generator Loss: 0.7201, Discriminator Loss: 1.2706\n",
      "Epoch [78/100], Step [20/32], Generator Loss: 0.6973, Discriminator Loss: 1.2596\n",
      "Epoch [78/100], Step [30/32], Generator Loss: 0.6839, Discriminator Loss: 1.2593\n",
      "Epoch [79/100], Step [0/32], Generator Loss: 0.6762, Discriminator Loss: 1.2828\n",
      "Epoch [79/100], Step [10/32], Generator Loss: 0.6791, Discriminator Loss: 1.2848\n",
      "Epoch [79/100], Step [20/32], Generator Loss: 0.6927, Discriminator Loss: 1.2415\n",
      "Epoch [79/100], Step [30/32], Generator Loss: 0.6710, Discriminator Loss: 1.2749\n",
      "Epoch [80/100], Step [0/32], Generator Loss: 0.6861, Discriminator Loss: 1.1790\n",
      "Epoch [80/100], Step [10/32], Generator Loss: 0.6692, Discriminator Loss: 1.1805\n",
      "Epoch [80/100], Step [20/32], Generator Loss: 0.7040, Discriminator Loss: 1.2238\n",
      "Epoch [80/100], Step [30/32], Generator Loss: 0.7202, Discriminator Loss: 1.2392\n",
      "Epoch [81/100], Step [0/32], Generator Loss: 0.7134, Discriminator Loss: 1.2291\n",
      "Epoch [81/100], Step [10/32], Generator Loss: 0.7134, Discriminator Loss: 1.1818\n",
      "Epoch [81/100], Step [20/32], Generator Loss: 0.7007, Discriminator Loss: 1.1631\n",
      "Epoch [81/100], Step [30/32], Generator Loss: 0.7048, Discriminator Loss: 1.1568\n",
      "Epoch [82/100], Step [0/32], Generator Loss: 0.7284, Discriminator Loss: 1.2264\n",
      "Epoch [82/100], Step [10/32], Generator Loss: 0.7476, Discriminator Loss: 1.2803\n",
      "Epoch [82/100], Step [20/32], Generator Loss: 0.7454, Discriminator Loss: 1.1969\n",
      "Epoch [82/100], Step [30/32], Generator Loss: 0.7211, Discriminator Loss: 1.1875\n",
      "Epoch [83/100], Step [0/32], Generator Loss: 0.7273, Discriminator Loss: 1.2088\n",
      "Epoch [83/100], Step [10/32], Generator Loss: 0.6785, Discriminator Loss: 1.2855\n",
      "Epoch [83/100], Step [20/32], Generator Loss: 0.6714, Discriminator Loss: 1.3262\n",
      "Epoch [83/100], Step [30/32], Generator Loss: 0.6910, Discriminator Loss: 1.1657\n",
      "Epoch [84/100], Step [0/32], Generator Loss: 0.6856, Discriminator Loss: 1.2457\n",
      "Epoch [84/100], Step [10/32], Generator Loss: 0.7115, Discriminator Loss: 1.1806\n",
      "Epoch [84/100], Step [20/32], Generator Loss: 0.7335, Discriminator Loss: 1.1561\n",
      "Epoch [84/100], Step [30/32], Generator Loss: 0.7350, Discriminator Loss: 1.1676\n",
      "Epoch [85/100], Step [0/32], Generator Loss: 0.7379, Discriminator Loss: 1.1336\n",
      "Epoch [85/100], Step [10/32], Generator Loss: 0.7260, Discriminator Loss: 1.1442\n",
      "Epoch [85/100], Step [20/32], Generator Loss: 0.7025, Discriminator Loss: 1.1367\n",
      "Epoch [85/100], Step [30/32], Generator Loss: 0.6782, Discriminator Loss: 1.1778\n",
      "Epoch [86/100], Step [0/32], Generator Loss: 0.6625, Discriminator Loss: 1.2301\n",
      "Epoch [86/100], Step [10/32], Generator Loss: 0.6354, Discriminator Loss: 1.2055\n",
      "Epoch [86/100], Step [20/32], Generator Loss: 0.6593, Discriminator Loss: 1.1715\n",
      "Epoch [86/100], Step [30/32], Generator Loss: 0.6568, Discriminator Loss: 1.1737\n",
      "Epoch [87/100], Step [0/32], Generator Loss: 0.6784, Discriminator Loss: 1.1487\n",
      "Epoch [87/100], Step [10/32], Generator Loss: 0.7010, Discriminator Loss: 1.1753\n",
      "Epoch [87/100], Step [20/32], Generator Loss: 0.7218, Discriminator Loss: 1.1650\n",
      "Epoch [87/100], Step [30/32], Generator Loss: 0.7010, Discriminator Loss: 1.1775\n",
      "Epoch [88/100], Step [0/32], Generator Loss: 0.7067, Discriminator Loss: 1.1369\n",
      "Epoch [88/100], Step [10/32], Generator Loss: 0.6804, Discriminator Loss: 1.2178\n",
      "Epoch [88/100], Step [20/32], Generator Loss: 0.7196, Discriminator Loss: 1.2690\n",
      "Epoch [88/100], Step [30/32], Generator Loss: 0.7096, Discriminator Loss: 1.2178\n",
      "Epoch [89/100], Step [0/32], Generator Loss: 0.7217, Discriminator Loss: 1.2476\n",
      "Epoch [89/100], Step [10/32], Generator Loss: 0.7668, Discriminator Loss: 1.1148\n",
      "Epoch [89/100], Step [20/32], Generator Loss: 0.7855, Discriminator Loss: 1.2105\n",
      "Epoch [89/100], Step [30/32], Generator Loss: 0.8206, Discriminator Loss: 1.0845\n",
      "Epoch [90/100], Step [0/32], Generator Loss: 0.8136, Discriminator Loss: 1.1897\n",
      "Epoch [90/100], Step [10/32], Generator Loss: 0.8240, Discriminator Loss: 1.0098\n",
      "Epoch [90/100], Step [20/32], Generator Loss: 0.8438, Discriminator Loss: 1.1638\n",
      "Epoch [90/100], Step [30/32], Generator Loss: 0.8049, Discriminator Loss: 1.1082\n",
      "Epoch [91/100], Step [0/32], Generator Loss: 0.8000, Discriminator Loss: 1.0618\n",
      "Epoch [91/100], Step [10/32], Generator Loss: 0.7777, Discriminator Loss: 1.0128\n",
      "Epoch [91/100], Step [20/32], Generator Loss: 0.7843, Discriminator Loss: 1.0871\n",
      "Epoch [91/100], Step [30/32], Generator Loss: 0.7880, Discriminator Loss: 1.0944\n",
      "Epoch [92/100], Step [0/32], Generator Loss: 0.7874, Discriminator Loss: 1.0653\n",
      "Epoch [92/100], Step [10/32], Generator Loss: 0.7609, Discriminator Loss: 1.1158\n",
      "Epoch [92/100], Step [20/32], Generator Loss: 0.7459, Discriminator Loss: 1.1894\n",
      "Epoch [92/100], Step [30/32], Generator Loss: 0.6914, Discriminator Loss: 1.0836\n",
      "Epoch [93/100], Step [0/32], Generator Loss: 0.6663, Discriminator Loss: 1.1638\n",
      "Epoch [93/100], Step [10/32], Generator Loss: 0.6634, Discriminator Loss: 1.1819\n",
      "Epoch [93/100], Step [20/32], Generator Loss: 0.6490, Discriminator Loss: 1.0694\n",
      "Epoch [93/100], Step [30/32], Generator Loss: 0.6516, Discriminator Loss: 1.1449\n",
      "Epoch [94/100], Step [0/32], Generator Loss: 0.6562, Discriminator Loss: 1.1868\n",
      "Epoch [94/100], Step [10/32], Generator Loss: 0.6665, Discriminator Loss: 1.1585\n",
      "Epoch [94/100], Step [20/32], Generator Loss: 0.6900, Discriminator Loss: 1.1270\n",
      "Epoch [94/100], Step [30/32], Generator Loss: 0.7212, Discriminator Loss: 1.1316\n",
      "Epoch [95/100], Step [0/32], Generator Loss: 0.7222, Discriminator Loss: 1.0680\n",
      "Epoch [95/100], Step [10/32], Generator Loss: 0.7232, Discriminator Loss: 1.0743\n",
      "Epoch [95/100], Step [20/32], Generator Loss: 0.7214, Discriminator Loss: 1.0438\n",
      "Epoch [95/100], Step [30/32], Generator Loss: 0.7279, Discriminator Loss: 1.1647\n",
      "Epoch [96/100], Step [0/32], Generator Loss: 0.7157, Discriminator Loss: 1.2347\n",
      "Epoch [96/100], Step [10/32], Generator Loss: 0.7047, Discriminator Loss: 1.1838\n",
      "Epoch [96/100], Step [20/32], Generator Loss: 0.6693, Discriminator Loss: 1.1142\n",
      "Epoch [96/100], Step [30/32], Generator Loss: 0.6734, Discriminator Loss: 1.3421\n",
      "Epoch [97/100], Step [0/32], Generator Loss: 0.6694, Discriminator Loss: 1.1708\n",
      "Epoch [97/100], Step [10/32], Generator Loss: 0.6756, Discriminator Loss: 1.2141\n",
      "Epoch [97/100], Step [20/32], Generator Loss: 0.7125, Discriminator Loss: 1.2687\n",
      "Epoch [97/100], Step [30/32], Generator Loss: 0.6843, Discriminator Loss: 1.1715\n",
      "Epoch [98/100], Step [0/32], Generator Loss: 0.6698, Discriminator Loss: 1.2004\n",
      "Epoch [98/100], Step [10/32], Generator Loss: 0.6631, Discriminator Loss: 1.2404\n",
      "Epoch [98/100], Step [20/32], Generator Loss: 0.6785, Discriminator Loss: 1.2229\n",
      "Epoch [98/100], Step [30/32], Generator Loss: 0.6760, Discriminator Loss: 1.2751\n",
      "Epoch [99/100], Step [0/32], Generator Loss: 0.6609, Discriminator Loss: 1.0729\n",
      "Epoch [99/100], Step [10/32], Generator Loss: 0.6536, Discriminator Loss: 1.2593\n",
      "Epoch [99/100], Step [20/32], Generator Loss: 0.6568, Discriminator Loss: 1.2243\n",
      "Epoch [99/100], Step [30/32], Generator Loss: 0.6656, Discriminator Loss: 1.1824\n",
      "Generated data (class label: 1 ):\n",
      "[[0.03455355 0.0233849  0.5963514  0.0327986  0.5756298  0.02978746\n",
      "  0.12818728 0.23626772 0.91506964 0.20587263]\n",
      " [0.03354738 0.02980399 0.9341956  0.12800264 0.7788775  0.03500072\n",
      "  0.09274872 0.04864118 0.5531642  0.19517064]\n",
      " [0.05685929 0.02286977 0.67009205 0.06521782 0.7099493  0.02344442\n",
      "  0.09455987 0.07168476 0.8667496  0.14675644]\n",
      " [0.04116518 0.22065887 0.9359485  0.20785451 0.97241837 0.0705671\n",
      "  0.08306287 0.08478554 0.6479532  0.12675606]\n",
      " [0.12684187 0.0439552  0.551216   0.01972515 0.19515516 0.08788622\n",
      "  0.18455108 0.22305894 0.95353055 0.33879572]\n",
      " [0.04548594 0.05840297 0.95466065 0.11065473 0.7959411  0.05948856\n",
      "  0.14279366 0.07989153 0.6500077  0.12709121]\n",
      " [0.16953397 0.04755177 0.6726334  0.03690435 0.4027058  0.1010228\n",
      "  0.2558814  0.2908014  0.93608284 0.22639704]\n",
      " [0.04864571 0.06608104 0.8600211  0.10980868 0.6161839  0.06458991\n",
      "  0.0518223  0.07974878 0.7159701  0.2828558 ]\n",
      " [0.03923905 0.08395199 0.9039575  0.11025965 0.7863873  0.09307241\n",
      "  0.07003763 0.064477   0.77137053 0.27069357]\n",
      " [0.01829252 0.08936843 0.9658716  0.28819647 0.9773062  0.07062443\n",
      "  0.05565353 0.06720409 0.5542412  0.09184868]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a simple cGAN for tabular data generation\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, input_size, num_classes, embedding_dim=16):\n",
    "        super(Generator, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_size + embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_size),\n",
    "            nn.Sigmoid()  # For scaling data between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        label_embedding = self.embedding(labels)\n",
    "        combined_input = torch.cat((z, label_embedding), dim=1)\n",
    "        return self.model(combined_input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, embedding_dim=16):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size + embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        label_embedding = self.embedding(labels)\n",
    "        combined_input = torch.cat((x, label_embedding), dim=1)\n",
    "        return self.model(combined_input)\n",
    "\n",
    "# Sample data generation (replace with your actual data loading logic)\n",
    "def generate_data(num_samples, input_size, num_classes):\n",
    "    # Assuming data is normally distributed within a specific range\n",
    "    data = np.random.rand(num_samples, input_size) * (upper_bound - lower_bound) + lower_bound\n",
    "    labels = np.random.randint(0, num_classes, size=(num_samples,))\n",
    "    return data, labels\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "input_size = 10  # Number of features in the data\n",
    "latent_size = 20  # Size of the input noise vector for the generator\n",
    "data_size = 1000  # Number of samples in the dataset\n",
    "num_classes = 2  # Number of classes in your data (adjust as needed)\n",
    "lower_bound = 0  # Lower bound for data values (adjust as needed)\n",
    "upper_bound = 1  # Upper bound for data values (adjust as needed)\n",
    "\n",
    "# Create generator and discriminator models\n",
    "generator = Generator(latent_size, input_size, num_classes)\n",
    "discriminator = Discriminator(input_size, num_classes)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Generate real tabular data\n",
    "real_data, real_labels = generate_data(data_size, input_size, num_classes)\n",
    "data_tensor = torch.from_numpy(np.concatenate((real_data, real_labels.reshape(-1, 1)), axis=1))\n",
    "data_loader = DataLoader(TensorDataset(data_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(data_loader):\n",
    "        real_samples, real_labels = data[0][:, :-1], data[0][:, -1]\n",
    "        batch_size = real_samples.size(0)\n",
    "\n",
    "        # Train Discriminator\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # Train with real data\n",
    "        real_output = discriminator(real_samples.float(), real_labels.long())\n",
    "        d_loss_real = criterion(real_output, torch.ones(batch_size, 1))\n",
    "\n",
    "        # Train with fake data\n",
    "        z = torch.randn(batch_size, latent_size)\n",
    "        fake_labels = torch.randint(0, num_classes, size=(batch_size,))  # Random labels for fake data (can be adjusted)\n",
    "        fake_samples = generator(z, fake_labels)\n",
    "        fake_output = discriminator(fake_samples.detach(), fake_labels)  # Detach to avoid backprop through G\n",
    "        d_loss_fake = criterion(fake_output, torch.zeros(batch_size, 1))\n",
    "\n",
    "        # Combined loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        generator.zero_grad()\n",
    "\n",
    "        # Generate fake data (with a specific class label for illustration)\n",
    "        specific_class_label = 1  # You can choose any class label here\n",
    "        z = torch.randn(batch_size, latent_size)\n",
    "        fake_samples = generator(z, torch.full((batch_size, ), specific_class_label))\n",
    "        fake_output = discriminator(fake_samples, torch.full((batch_size, ), specific_class_label))\n",
    "\n",
    "        # Generator loss (maximize log(D(G(z))))\n",
    "        g_loss = criterion(fake_output, torch.ones(batch_size, 1))\n",
    "        g_loss.backward(retain_graph=True)\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Print progress\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(data_loader)}], \"\n",
    "                  f\"Generator Loss: {g_loss.item():.4f}, Discriminator Loss: {d_loss.item():.4f}\")\n",
    "\n",
    "# Sample generated data (optional)\n",
    "def sample_data(generator, latent_size, num_samples, specific_class_label, device):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_size).to(device)\n",
    "        fake_labels = torch.full((num_samples, ), specific_class_label).to(device)\n",
    "        return generator(z, fake_labels)\n",
    "\n",
    "# Generate data with a specific class label (optional)\n",
    "specific_class_label = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generated_data = sample_data(generator.to(device), latent_size, 10, specific_class_label, device)\n",
    "print(\"Generated data (class label:\", specific_class_label, \"):\")\n",
    "print(generated_data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a10676-f137-46fe-bb5e-906868f530dd",
   "metadata": {},
   "source": [
    "# **3. Autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0606a40-03b8-4653-ae6d-18d09c7d8553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1, Loss: 32.25492477416992\n",
      "Epoch: 1, Batch: 101, Loss: 5.7697343826293945\n",
      "Epoch: 1, Batch: 201, Loss: 4.383053779602051\n",
      "Epoch: 1, Batch: 301, Loss: 4.34030818939209\n",
      "Epoch: 1, Batch: 401, Loss: 6.303532600402832\n",
      "Epoch: 1, Batch: 501, Loss: 6.092098236083984\n",
      "Epoch: 1, Batch: 601, Loss: 5.223959922790527\n",
      "Epoch: 1, Batch: 701, Loss: 5.139459609985352\n",
      "Epoch: 1, Batch: 801, Loss: 4.191755771636963\n",
      "Epoch: 1, Batch: 901, Loss: 4.849924087524414\n",
      "Epoch: 2, Batch: 1, Loss: 4.327437400817871\n",
      "Epoch: 2, Batch: 101, Loss: 4.087098121643066\n",
      "Epoch: 2, Batch: 201, Loss: 4.1558098793029785\n",
      "Epoch: 2, Batch: 301, Loss: 4.613105773925781\n",
      "Epoch: 2, Batch: 401, Loss: 5.213890552520752\n",
      "Epoch: 2, Batch: 501, Loss: 5.258609294891357\n",
      "Epoch: 2, Batch: 601, Loss: 5.1807146072387695\n",
      "Epoch: 2, Batch: 701, Loss: 4.436731815338135\n",
      "Epoch: 2, Batch: 801, Loss: 4.099480628967285\n",
      "Epoch: 2, Batch: 901, Loss: 4.4074811935424805\n",
      "Epoch: 3, Batch: 1, Loss: 4.249795913696289\n",
      "Epoch: 3, Batch: 101, Loss: 4.447699546813965\n",
      "Epoch: 3, Batch: 201, Loss: 3.5449748039245605\n",
      "Epoch: 3, Batch: 301, Loss: 4.506514072418213\n",
      "Epoch: 3, Batch: 401, Loss: 5.006190299987793\n",
      "Epoch: 3, Batch: 501, Loss: 4.532584190368652\n",
      "Epoch: 3, Batch: 601, Loss: 4.9886932373046875\n",
      "Epoch: 3, Batch: 701, Loss: 4.282009124755859\n",
      "Epoch: 3, Batch: 801, Loss: 3.8550827503204346\n",
      "Epoch: 3, Batch: 901, Loss: 4.139037132263184\n",
      "Epoch: 4, Batch: 1, Loss: 3.8485469818115234\n",
      "Epoch: 4, Batch: 101, Loss: 4.677953720092773\n",
      "Epoch: 4, Batch: 201, Loss: 3.11415433883667\n",
      "Epoch: 4, Batch: 301, Loss: 4.306360721588135\n",
      "Epoch: 4, Batch: 401, Loss: 4.963000297546387\n",
      "Epoch: 4, Batch: 501, Loss: 4.619687557220459\n",
      "Epoch: 4, Batch: 601, Loss: 4.825956344604492\n",
      "Epoch: 4, Batch: 701, Loss: 4.180004596710205\n",
      "Epoch: 4, Batch: 801, Loss: 3.6300158500671387\n",
      "Epoch: 4, Batch: 901, Loss: 4.245684623718262\n",
      "Epoch: 5, Batch: 1, Loss: 4.306246757507324\n",
      "Epoch: 5, Batch: 101, Loss: 4.573882102966309\n",
      "Epoch: 5, Batch: 201, Loss: 2.9678754806518555\n",
      "Epoch: 5, Batch: 301, Loss: 4.358847141265869\n",
      "Epoch: 5, Batch: 401, Loss: 4.855189800262451\n",
      "Epoch: 5, Batch: 501, Loss: 4.942202568054199\n",
      "Epoch: 5, Batch: 601, Loss: 4.558516979217529\n",
      "Epoch: 5, Batch: 701, Loss: 4.242629051208496\n",
      "Epoch: 5, Batch: 801, Loss: 3.3935635089874268\n",
      "Epoch: 5, Batch: 901, Loss: 3.914965867996216\n",
      "Epoch: 6, Batch: 1, Loss: 4.978963375091553\n",
      "Epoch: 6, Batch: 101, Loss: 4.7391510009765625\n",
      "Epoch: 6, Batch: 201, Loss: 2.918440818786621\n",
      "Epoch: 6, Batch: 301, Loss: 4.216193675994873\n",
      "Epoch: 6, Batch: 401, Loss: 4.58798360824585\n",
      "Epoch: 6, Batch: 501, Loss: 4.839834213256836\n",
      "Epoch: 6, Batch: 601, Loss: 4.409060478210449\n",
      "Epoch: 6, Batch: 701, Loss: 3.9937877655029297\n",
      "Epoch: 6, Batch: 801, Loss: 3.33709979057312\n",
      "Epoch: 6, Batch: 901, Loss: 4.1209540367126465\n",
      "Epoch: 7, Batch: 1, Loss: 5.252829551696777\n",
      "Epoch: 7, Batch: 101, Loss: 4.597418308258057\n",
      "Epoch: 7, Batch: 201, Loss: 2.857140302658081\n",
      "Epoch: 7, Batch: 301, Loss: 4.041646957397461\n",
      "Epoch: 7, Batch: 401, Loss: 4.6146016120910645\n",
      "Epoch: 7, Batch: 501, Loss: 4.558880805969238\n",
      "Epoch: 7, Batch: 601, Loss: 4.200003147125244\n",
      "Epoch: 7, Batch: 701, Loss: 4.080381870269775\n",
      "Epoch: 7, Batch: 801, Loss: 3.406784772872925\n",
      "Epoch: 7, Batch: 901, Loss: 4.069360256195068\n",
      "Epoch: 8, Batch: 1, Loss: 5.525667667388916\n",
      "Epoch: 8, Batch: 101, Loss: 4.610073566436768\n",
      "Epoch: 8, Batch: 201, Loss: 2.8304800987243652\n",
      "Epoch: 8, Batch: 301, Loss: 3.983534574508667\n",
      "Epoch: 8, Batch: 401, Loss: 4.529814720153809\n",
      "Epoch: 8, Batch: 501, Loss: 4.319919586181641\n",
      "Epoch: 8, Batch: 601, Loss: 4.235135078430176\n",
      "Epoch: 8, Batch: 701, Loss: 4.1450724601745605\n",
      "Epoch: 8, Batch: 801, Loss: 3.51045560836792\n",
      "Epoch: 8, Batch: 901, Loss: 4.100940227508545\n",
      "Epoch: 9, Batch: 1, Loss: 4.997881889343262\n",
      "Epoch: 9, Batch: 101, Loss: 4.456244468688965\n",
      "Epoch: 9, Batch: 201, Loss: 2.696343421936035\n",
      "Epoch: 9, Batch: 301, Loss: 3.6902763843536377\n",
      "Epoch: 9, Batch: 401, Loss: 4.296940803527832\n",
      "Epoch: 9, Batch: 501, Loss: 4.104118824005127\n",
      "Epoch: 9, Batch: 601, Loss: 3.805091381072998\n",
      "Epoch: 9, Batch: 701, Loss: 3.9172096252441406\n",
      "Epoch: 9, Batch: 801, Loss: 3.4648399353027344\n",
      "Epoch: 9, Batch: 901, Loss: 4.124442100524902\n",
      "Epoch: 10, Batch: 1, Loss: 4.984137058258057\n",
      "Epoch: 10, Batch: 101, Loss: 4.633460521697998\n",
      "Epoch: 10, Batch: 201, Loss: 2.8066751956939697\n",
      "Epoch: 10, Batch: 301, Loss: 3.6016101837158203\n",
      "Epoch: 10, Batch: 401, Loss: 4.241814136505127\n",
      "Epoch: 10, Batch: 501, Loss: 3.938781261444092\n",
      "Epoch: 10, Batch: 601, Loss: 3.8789234161376953\n",
      "Epoch: 10, Batch: 701, Loss: 3.6825942993164062\n",
      "Epoch: 10, Batch: 801, Loss: 3.500492811203003\n",
      "Epoch: 10, Batch: 901, Loss: 4.164425849914551\n",
      "Original Data:\n",
      "tensor([[3.0874e+00, 9.9493e+00, 3.9034e+00, 4.7466e+00, 6.3923e-01, 1.5251e+00,\n",
      "         6.8833e+00, 5.1362e+00, 2.6148e+00, 6.3872e+00],\n",
      "        [4.9724e+00, 5.7262e+00, 8.4400e+00, 7.7005e+00, 1.4355e+00, 7.9138e-01,\n",
      "         4.7816e+00, 3.4737e+00, 6.1597e-01, 1.3244e+00],\n",
      "        [3.3717e+00, 9.5065e+00, 8.5954e+00, 9.5670e+00, 4.9859e+00, 2.3723e+00,\n",
      "         3.3694e+00, 3.7403e+00, 8.7620e+00, 8.1091e+00],\n",
      "        [9.5771e+00, 6.1757e+00, 6.3248e+00, 9.7465e+00, 1.7475e+00, 4.5512e+00,\n",
      "         4.9775e-01, 8.0109e+00, 4.6032e+00, 9.7662e+00],\n",
      "        [2.3085e+00, 5.7052e+00, 6.5509e+00, 9.1794e+00, 1.6507e+00, 2.6663e+00,\n",
      "         1.3554e+00, 9.2999e+00, 7.2926e+00, 4.1559e+00],\n",
      "        [7.2738e+00, 3.1544e+00, 7.7994e+00, 8.4355e+00, 6.6114e+00, 5.2836e-01,\n",
      "         3.0440e+00, 5.0171e-01, 5.4655e-01, 1.4525e+00],\n",
      "        [7.1124e+00, 7.3662e+00, 8.9721e-01, 3.4879e+00, 7.6644e+00, 4.4869e+00,\n",
      "         1.2523e+00, 7.2807e+00, 8.4768e+00, 7.7136e+00],\n",
      "        [4.6230e+00, 3.9475e+00, 2.5534e+00, 8.9460e+00, 9.0443e+00, 6.0850e+00,\n",
      "         1.5394e+00, 2.1625e+00, 1.4459e+00, 9.3195e+00],\n",
      "        [3.6020e+00, 9.3869e+00, 8.9376e+00, 1.4721e+00, 4.3021e+00, 8.4269e-01,\n",
      "         4.8670e+00, 8.5011e-01, 1.2106e+00, 7.3862e+00],\n",
      "        [4.3600e+00, 1.8269e+00, 1.8055e+00, 9.4998e-01, 3.8407e+00, 6.9289e+00,\n",
      "         8.4462e-01, 9.3979e+00, 4.1626e-01, 5.7377e+00],\n",
      "        [6.2718e+00, 8.5874e-01, 1.1930e-01, 4.5096e+00, 3.5776e+00, 1.0925e+00,\n",
      "         9.9871e+00, 1.7490e+00, 3.8788e+00, 8.7859e+00],\n",
      "        [5.0519e+00, 3.4610e+00, 5.8848e+00, 9.4737e+00, 5.5875e+00, 7.9386e+00,\n",
      "         8.1411e+00, 4.6280e+00, 2.1611e+00, 3.7849e+00],\n",
      "        [5.2034e+00, 1.3719e+00, 8.5289e+00, 7.7385e+00, 9.3132e+00, 5.9643e+00,\n",
      "         3.2120e+00, 2.4421e+00, 7.8344e+00, 1.4932e+00],\n",
      "        [2.5897e+00, 1.5923e+00, 6.7138e+00, 5.8217e+00, 1.8545e+00, 9.9207e+00,\n",
      "         9.0923e+00, 2.3963e+00, 1.4003e+00, 9.9186e+00],\n",
      "        [4.5411e+00, 5.9071e+00, 3.8952e-01, 3.6439e-01, 6.1635e+00, 4.1150e+00,\n",
      "         4.1855e-01, 7.2228e+00, 3.2828e+00, 8.2280e+00],\n",
      "        [5.8213e-01, 6.3704e+00, 2.1316e+00, 3.2377e+00, 7.5941e+00, 3.8857e+00,\n",
      "         9.2614e-01, 9.5176e+00, 1.3046e+00, 6.7674e+00],\n",
      "        [3.9203e+00, 6.9042e+00, 6.2221e+00, 4.6058e+00, 4.3189e+00, 4.0580e-01,\n",
      "         1.9037e+00, 8.3107e+00, 8.8103e+00, 9.1812e+00],\n",
      "        [1.8142e+00, 4.0210e+00, 9.8216e+00, 6.1027e+00, 5.6434e+00, 2.1358e+00,\n",
      "         4.1363e+00, 5.4410e+00, 1.3179e-01, 3.2611e+00],\n",
      "        [5.7348e+00, 8.4941e+00, 3.6880e+00, 9.2876e+00, 5.2481e+00, 1.3905e+00,\n",
      "         4.5608e-01, 8.5459e+00, 7.3241e+00, 9.2931e+00],\n",
      "        [4.1430e+00, 6.3214e-01, 2.1399e+00, 5.0163e+00, 7.5644e+00, 2.3935e+00,\n",
      "         6.0627e+00, 2.2364e+00, 3.8190e+00, 2.7975e+00],\n",
      "        [9.2677e-02, 3.0098e+00, 9.6339e+00, 1.6792e+00, 7.3508e+00, 5.9793e+00,\n",
      "         2.8127e+00, 8.5242e+00, 5.0397e-01, 4.1920e-01],\n",
      "        [8.4353e+00, 9.7075e+00, 7.5768e+00, 5.8702e+00, 7.1985e+00, 9.0195e+00,\n",
      "         3.8947e+00, 1.0365e+00, 4.4858e+00, 6.9207e+00],\n",
      "        [7.8032e+00, 2.9327e+00, 2.5306e-02, 6.8964e+00, 8.1383e+00, 1.7880e+00,\n",
      "         1.7489e+00, 9.4816e+00, 5.9852e+00, 8.8717e+00],\n",
      "        [4.1865e+00, 7.4156e+00, 1.8075e+00, 4.5209e+00, 7.8576e-01, 3.0224e-01,\n",
      "         8.2164e+00, 7.9151e+00, 3.0382e-01, 7.3594e+00],\n",
      "        [8.0604e+00, 9.8210e+00, 9.1570e+00, 9.0345e-01, 3.7390e+00, 6.5792e+00,\n",
      "         6.8578e-01, 6.0970e+00, 1.8912e+00, 8.4497e+00],\n",
      "        [4.8627e+00, 1.9807e+00, 9.1986e+00, 4.4812e+00, 3.8216e+00, 1.3253e+00,\n",
      "         1.1353e+00, 7.7442e+00, 1.5496e+00, 5.2296e+00],\n",
      "        [3.8067e+00, 2.5405e+00, 9.4173e+00, 2.4284e+00, 2.3697e+00, 7.8222e+00,\n",
      "         7.6834e+00, 4.3378e+00, 8.2404e+00, 2.3936e+00],\n",
      "        [8.1698e+00, 4.3652e+00, 1.7871e-01, 9.3169e+00, 9.2556e+00, 3.9225e+00,\n",
      "         8.7009e-01, 6.3175e+00, 1.8523e+00, 3.8222e+00],\n",
      "        [2.8837e+00, 2.8876e-01, 4.2147e+00, 1.6364e+00, 8.6434e-02, 6.3606e+00,\n",
      "         1.7103e-01, 5.3007e+00, 1.6863e+00, 1.8195e+00],\n",
      "        [2.6457e+00, 3.7128e+00, 1.4043e+00, 8.6795e+00, 3.3191e+00, 1.7903e+00,\n",
      "         5.2167e-01, 2.3809e+00, 1.9598e+00, 3.8865e-01],\n",
      "        [6.1939e+00, 9.5247e+00, 9.5815e+00, 3.7015e+00, 7.2760e+00, 8.5518e-01,\n",
      "         8.1378e+00, 6.4545e-01, 5.2037e+00, 6.8861e+00],\n",
      "        [7.3779e+00, 3.3898e+00, 8.8696e+00, 1.2963e+00, 9.8346e+00, 1.4211e+00,\n",
      "         6.9670e+00, 9.6889e+00, 7.6184e+00, 4.6113e-01],\n",
      "        [3.8528e+00, 8.7905e+00, 9.9986e+00, 9.1786e+00, 7.5034e+00, 6.4616e+00,\n",
      "         4.1836e+00, 7.5092e+00, 5.3517e+00, 8.4778e+00],\n",
      "        [5.2747e+00, 9.0274e+00, 6.0608e+00, 1.7647e-01, 9.1101e+00, 6.7478e+00,\n",
      "         9.5638e-01, 5.0823e+00, 2.6050e+00, 6.9336e+00],\n",
      "        [1.9403e+00, 9.0514e+00, 9.5807e+00, 8.3482e+00, 9.6944e+00, 6.1969e+00,\n",
      "         8.7109e+00, 4.3130e+00, 8.3375e+00, 8.5192e+00],\n",
      "        [2.3376e+00, 5.8201e+00, 5.4757e+00, 7.9249e+00, 6.0340e+00, 3.2316e+00,\n",
      "         3.6723e+00, 6.8964e+00, 5.3635e+00, 6.7787e+00],\n",
      "        [1.8053e+00, 5.1319e+00, 4.4493e+00, 3.9089e-02, 2.7898e+00, 6.0352e+00,\n",
      "         6.4201e+00, 8.0553e+00, 6.4203e+00, 3.6686e+00],\n",
      "        [9.3861e+00, 5.7827e+00, 5.4312e+00, 8.3650e+00, 7.1098e+00, 4.4694e+00,\n",
      "         7.2169e+00, 2.9127e+00, 9.6028e+00, 6.6752e-01],\n",
      "        [4.0759e+00, 3.8386e+00, 4.9663e+00, 6.0681e+00, 5.1944e+00, 2.1260e+00,\n",
      "         2.8672e+00, 5.9248e-01, 4.5189e+00, 1.5940e+00],\n",
      "        [1.0909e+00, 4.3882e+00, 4.1564e+00, 1.3895e+00, 4.3361e+00, 4.7388e+00,\n",
      "         5.6022e+00, 5.5445e+00, 8.7872e+00, 2.8692e+00],\n",
      "        [9.0753e-01, 8.1472e+00, 9.6158e-01, 3.0265e+00, 9.9470e+00, 7.9686e+00,\n",
      "         6.7889e+00, 3.1895e+00, 4.8202e+00, 3.4674e+00],\n",
      "        [9.8419e+00, 6.2446e+00, 5.7448e+00, 6.6710e+00, 8.1483e+00, 3.4213e-01,\n",
      "         1.5162e-01, 6.7886e-01, 9.2201e+00, 1.0880e+00],\n",
      "        [7.5801e+00, 7.8065e+00, 9.1733e+00, 3.3893e+00, 1.7745e+00, 3.7807e+00,\n",
      "         1.8641e+00, 3.1536e+00, 4.8665e+00, 8.8570e+00],\n",
      "        [5.2528e+00, 8.5736e+00, 1.6513e+00, 3.1702e+00, 4.0900e+00, 1.6776e+00,\n",
      "         9.4313e+00, 1.8179e-01, 7.1673e+00, 1.9946e+00],\n",
      "        [3.6486e+00, 9.5772e+00, 9.8772e+00, 5.7854e+00, 5.2539e+00, 6.5680e+00,\n",
      "         8.6800e+00, 6.6719e+00, 6.4959e+00, 1.4649e+00],\n",
      "        [9.9972e+00, 3.6670e+00, 5.1696e+00, 3.5374e+00, 2.4895e+00, 5.5161e-01,\n",
      "         8.3946e-01, 4.1035e+00, 9.4697e+00, 1.9222e+00],\n",
      "        [9.8658e-01, 1.4578e-01, 2.5480e+00, 5.1285e+00, 5.5950e+00, 9.5537e+00,\n",
      "         4.8178e+00, 3.9500e+00, 6.8213e+00, 3.1966e-01],\n",
      "        [8.8848e-02, 1.7270e+00, 5.7076e+00, 7.3948e+00, 6.9674e+00, 5.6669e+00,\n",
      "         8.7654e+00, 8.7490e+00, 2.1417e+00, 3.7543e+00],\n",
      "        [8.8393e+00, 1.7485e+00, 9.7588e+00, 5.5382e-01, 5.1252e+00, 6.1187e+00,\n",
      "         6.7586e+00, 3.3230e+00, 6.1080e-02, 7.3691e+00],\n",
      "        [6.4411e+00, 9.0956e+00, 3.4654e+00, 2.4782e+00, 3.2957e+00, 4.5018e+00,\n",
      "         6.0296e+00, 1.6080e+00, 9.8205e+00, 8.3229e+00],\n",
      "        [5.7961e+00, 2.1910e+00, 7.9865e+00, 1.2284e+00, 6.3997e+00, 3.6993e+00,\n",
      "         2.7146e-02, 4.0674e+00, 3.4349e+00, 1.3125e+00],\n",
      "        [2.8322e+00, 9.0888e+00, 6.8123e+00, 2.8074e+00, 8.1067e+00, 6.7613e+00,\n",
      "         9.3883e+00, 9.6003e+00, 8.1556e+00, 7.7625e+00],\n",
      "        [7.7751e+00, 6.2001e+00, 1.0602e+00, 9.1913e+00, 6.9044e+00, 1.1216e+00,\n",
      "         1.2197e+00, 9.1453e+00, 1.4938e+00, 5.5583e+00],\n",
      "        [2.1977e-01, 2.0766e+00, 5.6299e+00, 7.8435e+00, 7.3682e-01, 1.4627e-01,\n",
      "         1.1276e+00, 5.9408e+00, 8.5497e+00, 2.8164e+00],\n",
      "        [3.0810e+00, 7.3629e+00, 2.8477e+00, 4.2026e+00, 9.2869e+00, 3.5229e+00,\n",
      "         8.4093e+00, 5.5976e+00, 4.5836e+00, 6.9972e+00],\n",
      "        [3.0481e+00, 5.2543e+00, 4.4214e+00, 1.7155e+00, 9.0447e+00, 8.7535e+00,\n",
      "         9.9221e+00, 1.9924e+00, 6.2950e+00, 2.3405e+00],\n",
      "        [1.5087e-01, 1.8925e+00, 3.0149e+00, 2.5634e+00, 3.6055e+00, 2.0092e+00,\n",
      "         1.2554e+00, 5.3024e+00, 7.2153e+00, 7.6551e+00],\n",
      "        [5.3381e+00, 4.0026e+00, 8.3357e+00, 9.8229e+00, 1.0685e+00, 1.6123e+00,\n",
      "         4.6073e+00, 1.6712e+00, 1.3851e+00, 3.3599e+00],\n",
      "        [9.5254e+00, 3.3643e+00, 1.2145e+00, 4.2156e+00, 9.7408e-01, 4.4042e+00,\n",
      "         9.7150e+00, 4.9477e+00, 5.8261e+00, 9.1037e+00],\n",
      "        [2.6797e+00, 7.6023e+00, 3.9603e-01, 1.9851e+00, 5.0776e+00, 8.3930e+00,\n",
      "         7.0913e+00, 2.2533e+00, 6.2171e-01, 6.0744e+00],\n",
      "        [8.0357e+00, 1.5466e+00, 6.5104e+00, 2.3977e+00, 7.0114e+00, 1.7750e+00,\n",
      "         2.9972e+00, 3.4897e+00, 7.7577e+00, 8.7955e+00],\n",
      "        [2.5697e+00, 4.2379e+00, 2.3698e+00, 1.6338e+00, 1.9238e+00, 8.0957e+00,\n",
      "         2.1509e+00, 1.6525e-01, 6.0261e+00, 2.2273e+00],\n",
      "        [3.3874e+00, 5.8470e+00, 3.9597e-01, 7.1029e+00, 4.6515e+00, 6.8385e+00,\n",
      "         3.4802e-01, 4.1245e+00, 7.8324e+00, 8.3325e+00],\n",
      "        [9.6006e+00, 7.2052e+00, 9.5648e+00, 4.3714e+00, 9.6532e+00, 5.5604e+00,\n",
      "         8.0529e+00, 3.3260e-02, 6.1107e+00, 9.5236e+00],\n",
      "        [8.3161e+00, 8.8470e+00, 9.9735e+00, 1.7972e+00, 1.0916e+00, 4.0076e+00,\n",
      "         3.6951e+00, 6.7489e+00, 4.6185e+00, 4.4860e-01],\n",
      "        [4.9575e+00, 6.5350e+00, 4.5036e+00, 8.1985e+00, 9.3934e+00, 2.6917e+00,\n",
      "         7.0168e+00, 2.8489e+00, 2.4469e+00, 8.8939e+00],\n",
      "        [8.7242e+00, 8.4953e-01, 1.1584e+00, 3.4830e+00, 5.4396e+00, 7.0170e+00,\n",
      "         3.7855e+00, 5.5396e+00, 9.8640e+00, 2.9888e+00],\n",
      "        [4.9115e+00, 4.3772e+00, 1.8129e+00, 4.6804e+00, 4.3957e+00, 3.9514e+00,\n",
      "         1.1474e+00, 5.6523e+00, 9.5071e+00, 6.5774e+00],\n",
      "        [6.3227e+00, 2.7023e+00, 7.9817e+00, 8.2714e+00, 4.0450e+00, 7.4370e+00,\n",
      "         5.1975e-01, 3.1655e+00, 6.1071e+00, 2.5962e+00],\n",
      "        [7.0475e+00, 8.4456e+00, 6.4061e+00, 1.5794e+00, 5.8577e+00, 1.6594e+00,\n",
      "         4.5625e+00, 6.6551e+00, 3.6591e+00, 7.1019e+00],\n",
      "        [7.4572e+00, 4.8943e-01, 9.5956e+00, 1.9222e-03, 3.1121e+00, 9.5888e+00,\n",
      "         1.2410e+00, 4.9391e+00, 5.7889e+00, 6.9759e+00],\n",
      "        [1.6021e+00, 6.9301e+00, 1.4426e+00, 5.9047e+00, 9.5593e+00, 6.0492e+00,\n",
      "         2.0804e+00, 1.1166e+00, 2.6730e+00, 3.3568e+00],\n",
      "        [9.3279e+00, 3.4480e+00, 7.1968e+00, 6.9224e+00, 4.1496e+00, 7.7212e+00,\n",
      "         7.9968e+00, 8.6103e+00, 8.4832e+00, 7.9256e+00],\n",
      "        [2.3013e+00, 5.1573e+00, 1.6165e-01, 1.2033e+00, 6.5984e+00, 7.3115e+00,\n",
      "         1.5975e+00, 3.7804e-02, 3.7418e+00, 3.3933e+00],\n",
      "        [5.4631e+00, 7.8101e+00, 5.0392e-01, 5.3387e+00, 7.4329e+00, 7.8317e+00,\n",
      "         1.0631e+00, 9.0421e+00, 6.6433e+00, 4.7399e+00],\n",
      "        [7.2970e+00, 7.2439e+00, 7.4760e+00, 7.0397e-01, 8.8866e-01, 3.3497e+00,\n",
      "         2.1756e+00, 5.4084e-01, 9.6100e+00, 3.3451e+00],\n",
      "        [6.7534e+00, 6.5767e+00, 1.6354e+00, 8.6697e+00, 1.9242e+00, 8.2996e+00,\n",
      "         7.9239e+00, 9.9824e+00, 6.9156e+00, 3.3532e-01],\n",
      "        [3.5328e+00, 2.8851e+00, 2.9985e+00, 9.5032e+00, 7.0722e+00, 8.4427e+00,\n",
      "         5.8754e+00, 9.0314e+00, 6.8593e+00, 7.9415e+00],\n",
      "        [1.7997e+00, 8.6502e+00, 9.1943e+00, 5.1437e+00, 1.2373e+00, 9.3918e+00,\n",
      "         9.8093e+00, 8.6920e+00, 4.8102e+00, 1.2337e-01],\n",
      "        [7.2182e+00, 1.2565e+00, 5.9360e+00, 1.5867e+00, 4.3695e+00, 8.8936e+00,\n",
      "         1.6542e+00, 8.4713e+00, 3.3832e+00, 2.9030e+00],\n",
      "        [8.6501e-01, 3.2200e+00, 4.4279e-01, 4.9470e+00, 3.7778e+00, 8.5302e+00,\n",
      "         9.6769e+00, 9.3794e+00, 1.5074e+00, 5.1898e+00],\n",
      "        [1.2561e+00, 3.4840e+00, 1.7425e+00, 9.2318e+00, 9.8283e+00, 2.0147e-01,\n",
      "         8.6915e+00, 9.3822e+00, 2.5524e+00, 1.7538e-01],\n",
      "        [9.0191e+00, 3.5454e+00, 9.9382e+00, 3.9853e+00, 6.6263e+00, 6.6533e+00,\n",
      "         5.4072e+00, 6.3774e+00, 9.5134e-01, 1.6425e+00],\n",
      "        [9.9140e+00, 4.3705e+00, 8.6289e+00, 7.9249e-01, 7.7159e+00, 4.1381e+00,\n",
      "         2.7034e-01, 2.1899e+00, 1.9395e+00, 8.6430e+00],\n",
      "        [9.1269e+00, 6.0343e+00, 8.2710e+00, 5.4627e+00, 9.6346e-01, 6.1864e+00,\n",
      "         5.2365e-01, 3.7443e+00, 2.0011e+00, 9.8745e+00],\n",
      "        [5.9092e+00, 2.9356e+00, 5.4724e+00, 9.9405e+00, 1.5265e+00, 5.3527e+00,\n",
      "         1.4057e-01, 3.4386e+00, 2.0408e+00, 5.2672e+00],\n",
      "        [5.2850e+00, 7.1356e+00, 1.6609e+00, 4.8735e+00, 3.1761e+00, 9.2346e+00,\n",
      "         1.0827e+00, 7.9084e+00, 2.3613e+00, 1.3284e+00],\n",
      "        [8.7233e+00, 8.4496e+00, 9.1122e+00, 8.1356e+00, 9.9348e-02, 7.3760e+00,\n",
      "         2.3829e-01, 7.2300e+00, 6.4562e+00, 5.5192e+00],\n",
      "        [3.3321e+00, 8.3167e+00, 1.4550e+00, 9.9039e-01, 7.3004e+00, 8.3122e+00,\n",
      "         2.2758e+00, 4.6025e+00, 4.5836e+00, 5.6219e+00],\n",
      "        [5.5498e+00, 1.7943e+00, 8.6975e+00, 2.6903e+00, 4.4764e+00, 4.0951e+00,\n",
      "         2.3761e+00, 3.0241e+00, 2.5268e+00, 9.6329e+00],\n",
      "        [2.1725e+00, 5.8706e+00, 1.4021e+00, 7.4132e+00, 6.0453e+00, 1.9433e+00,\n",
      "         1.5306e+00, 2.1755e+00, 8.8982e+00, 1.5852e-02],\n",
      "        [1.2102e+00, 1.8300e+00, 1.4222e+00, 5.0508e+00, 4.9278e+00, 7.9201e+00,\n",
      "         2.0887e+00, 1.5224e+00, 8.1652e+00, 9.3039e+00],\n",
      "        [9.2863e+00, 3.7452e+00, 7.0434e+00, 4.5145e+00, 2.2689e-01, 7.2029e+00,\n",
      "         3.7365e+00, 4.0250e+00, 7.6672e+00, 4.1044e+00],\n",
      "        [3.7975e+00, 7.9873e+00, 8.7305e+00, 8.3578e+00, 7.1433e+00, 8.7801e-01,\n",
      "         2.1399e+00, 2.6713e+00, 5.8120e+00, 6.3435e+00],\n",
      "        [4.9025e+00, 5.7542e+00, 8.1804e-02, 1.8014e+00, 5.5332e+00, 4.6447e+00,\n",
      "         7.8728e+00, 7.9917e+00, 1.7576e+00, 3.2007e+00],\n",
      "        [3.1587e-01, 9.5988e+00, 5.3827e+00, 2.5624e+00, 9.2973e+00, 6.7222e+00,\n",
      "         3.2803e+00, 7.4238e+00, 8.4110e+00, 2.7790e+00],\n",
      "        [1.6387e+00, 4.8758e+00, 7.5950e+00, 6.5250e+00, 9.3498e+00, 6.2970e+00,\n",
      "         1.1205e+00, 9.2355e+00, 5.3819e+00, 5.2621e+00],\n",
      "        [6.1928e+00, 6.9072e-01, 7.5595e+00, 4.2385e+00, 3.7338e+00, 3.1569e+00,\n",
      "         6.9895e+00, 7.9120e+00, 4.7225e+00, 8.5579e+00],\n",
      "        [2.9000e+00, 9.1447e+00, 2.1957e+00, 9.5257e+00, 9.2920e+00, 3.4174e+00,\n",
      "         4.3017e+00, 4.8602e+00, 6.7120e+00, 2.0189e-01],\n",
      "        [7.4098e+00, 6.6623e+00, 4.3373e+00, 8.0370e+00, 1.4130e+00, 3.8931e+00,\n",
      "         9.1514e+00, 9.2290e+00, 7.7572e+00, 8.8820e+00]])\n",
      "Augmented Data:\n",
      "tensor([[ 4.1465,  7.3034,  4.2260,  5.2959,  3.4988,  3.3253,  6.5115,  5.9066,\n",
      "          4.8274,  5.6574],\n",
      "        [ 3.5555,  5.2598,  7.3952,  4.3017,  4.4818,  4.7496,  4.6551,  4.5276,\n",
      "          3.6827,  4.0256],\n",
      "        [ 6.6227,  7.4025,  7.3196,  4.8577,  4.1645,  2.2857,  7.3206,  4.5421,\n",
      "          6.9838,  7.2590],\n",
      "        [ 6.8797,  7.4176,  2.0352,  6.0467,  2.2281,  4.1921,  6.0283,  6.0763,\n",
      "          5.6025,  7.2892],\n",
      "        [ 3.1367,  6.3599,  4.3637,  4.7052,  4.2249,  2.6562,  6.3380,  5.4964,\n",
      "          4.8543,  4.8930],\n",
      "        [ 8.9508,  1.9370,  9.1382,  6.3702,  5.2476,  4.8560,  3.2760,  2.0581,\n",
      "          2.2490,  1.0470],\n",
      "        [ 5.5147,  6.5842,  2.1131,  6.3453,  2.3237,  4.8774,  5.0047,  6.3968,\n",
      "          4.6224,  6.4110],\n",
      "        [ 1.9660,  4.8140,  2.2422,  3.9991,  3.6346,  6.0386,  4.2611,  4.6000,\n",
      "          3.0543,  3.8563],\n",
      "        [ 3.4345,  5.5989,  6.8390,  4.8555,  4.7627,  2.8455,  6.2802,  5.1384,\n",
      "          4.7528,  5.0933],\n",
      "        [ 2.5758,  3.3557,  1.2057,  5.2570,  3.6993,  4.4098,  4.1039,  6.0877,\n",
      "          3.3503,  4.0163],\n",
      "        [ 9.5574,  2.8836, -0.3144,  2.3073,  2.0915,  0.5580,  8.5734,  2.6274,\n",
      "          5.9880, 10.4365],\n",
      "        [ 3.5535,  6.9935,  6.5557,  3.3576,  5.8063,  7.8944,  7.4505,  4.7448,\n",
      "          4.1770,  3.9459],\n",
      "        [ 6.7798,  3.2204,  8.3796,  5.7653,  5.4346,  5.5819,  3.8561,  3.5164,\n",
      "          2.8813,  3.3450],\n",
      "        [ 2.7837,  6.1354,  4.7129,  2.5292,  5.7774,  8.5757,  5.2069,  3.2565,\n",
      "          4.8091,  5.0542],\n",
      "        [ 2.7635,  4.0083,  1.6694,  5.6855,  3.0686,  7.0777,  3.2950,  5.7354,\n",
      "          2.8709,  5.0811],\n",
      "        [ 3.2719,  4.7988,  1.6300,  6.0053,  3.7091,  4.7569,  5.2008,  6.9025,\n",
      "          3.6645,  4.9824],\n",
      "        [ 5.3323,  4.7649,  4.7988,  2.8712,  5.4286,  0.7219,  7.2711,  3.1351,\n",
      "          8.5099,  8.0469],\n",
      "        [ 3.8002,  2.2745,  8.5980,  4.7211,  5.2482,  4.5164,  6.3177,  4.3643,\n",
      "          2.9538,  3.0127],\n",
      "        [ 6.7914,  6.5120,  1.5330,  4.9528,  2.8820,  1.9869,  7.6950,  4.9383,\n",
      "          6.4172,  8.2756],\n",
      "        [ 2.2061,  2.9874,  6.0670,  3.4323,  4.9864,  2.7477,  5.3181,  3.9918,\n",
      "          4.3661,  3.2407],\n",
      "        [ 2.8925,  2.5085,  9.5300,  3.9291,  4.8582,  3.7747,  5.4862,  4.9727,\n",
      "          4.1814,  1.3002],\n",
      "        [ 5.6072,  9.0748,  7.8938,  4.4490,  5.8611,  9.4444,  5.5461,  5.4841,\n",
      "          5.6404,  5.4056],\n",
      "        [ 8.5474,  1.8046,  0.3873,  4.3517,  5.2319,  3.1415,  5.7266,  8.1725,\n",
      "          8.1384,  7.6757],\n",
      "        [ 4.8273,  5.7650,  1.8833,  5.8587,  2.8066,  3.2189,  5.9933,  5.9362,\n",
      "          4.6603,  6.6123],\n",
      "        [ 4.9658,  8.5021,  7.7526,  4.5118,  5.3191,  8.2771,  5.5380,  5.5405,\n",
      "          5.0632,  4.9081],\n",
      "        [ 2.7103,  2.1183,  9.5742,  4.1980,  6.5556,  2.7785,  5.1105,  5.3596,\n",
      "          5.1698,  3.0565],\n",
      "        [ 4.3211,  2.3572,  7.0920,  1.7925,  4.4813,  8.5282,  6.2429,  3.6950,\n",
      "          6.7729,  2.0353],\n",
      "        [ 5.2177,  6.3627,  6.3750,  3.6132,  3.5570,  5.2663,  1.4350,  3.3962,\n",
      "          5.5398,  2.9388],\n",
      "        [ 1.5704,  1.2292,  2.4225,  3.9974,  1.7850,  5.5464,  1.1158,  3.4288,\n",
      "          3.6998,  1.4134],\n",
      "        [ 2.2162,  3.0648,  1.6790,  4.2796,  2.5695,  4.6240,  1.7478,  3.9471,\n",
      "          3.0496,  3.3945],\n",
      "        [ 5.8698,  6.1251,  7.8364,  5.3216,  5.0165,  1.9522,  7.7007,  4.7882,\n",
      "          6.4921,  7.3197],\n",
      "        [ 4.3404,  4.4675, 10.6437, -2.3469,  8.3555,  3.2378,  9.8224, 10.5162,\n",
      "         12.1855,  4.7367],\n",
      "        [ 5.7907,  8.1955,  9.4916,  5.6860,  6.3601,  7.2903,  7.8628,  6.5431,\n",
      "          4.3726,  5.8891],\n",
      "        [ 4.2567,  7.8445,  6.7634,  3.4893,  5.6202,  8.9359,  4.5819,  4.3131,\n",
      "          5.2303,  4.6326],\n",
      "        [ 6.0972,  9.1802,  8.7123,  5.4889,  7.2698,  5.7038,  9.4281,  7.6904,\n",
      "          6.4989,  5.7040],\n",
      "        [ 3.2186,  6.8610,  4.7208,  4.8068,  4.2905,  3.2145,  6.5844,  5.8108,\n",
      "          4.6346,  4.7899],\n",
      "        [ 2.6775,  5.9255,  3.2802,  2.5502,  5.3376,  5.7814,  6.4529,  4.5686,\n",
      "          4.6844,  2.8274],\n",
      "        [ 4.2097,  6.4584,  7.4479,  5.4534,  5.6487,  2.7726,  7.4647,  6.1260,\n",
      "          5.7748,  5.7595],\n",
      "        [ 4.2967,  3.0770,  5.9430,  3.6717,  3.7354,  3.0657,  2.6952,  2.2418,\n",
      "          4.5274,  3.1547],\n",
      "        [ 3.3144,  5.2487,  2.5972,  2.2217,  6.3510,  4.3328,  6.4099,  5.1902,\n",
      "          6.0273,  1.9619],\n",
      "        [ 2.8650,  4.3818,  2.6327,  2.3626,  5.8781,  8.6052,  5.9615,  3.9918,\n",
      "          4.8853,  4.9867],\n",
      "        [12.6436,  4.6795,  3.7208,  2.6345,  2.8852,  0.7096,  4.9869, -1.3493,\n",
      "          8.0401,  4.1674],\n",
      "        [ 5.5661,  7.1991,  6.3748,  4.2557,  3.3003,  3.5710,  4.3346,  4.1552,\n",
      "          5.9936,  5.1140],\n",
      "        [ 3.7298,  5.9125,  3.7214,  4.4947,  4.7949,  1.7237,  6.5039,  5.3362,\n",
      "          6.2389,  5.5670],\n",
      "        [ 4.7517,  8.0404,  7.8705,  4.4943,  6.6601,  6.4186,  8.3788,  6.3882,\n",
      "          5.3501,  4.7309],\n",
      "        [ 6.3266,  3.7475,  2.8206,  0.8710,  3.2761,  0.2391,  5.6926,  0.8487,\n",
      "          7.4505,  7.1782],\n",
      "        [ 3.7094,  0.4571,  2.5203,  3.6093,  2.8371,  9.3101,  2.5533,  4.4689,\n",
      "          7.8242,  2.9856],\n",
      "        [ 2.9488,  6.7325,  4.4791,  3.5268,  5.1931,  4.6460,  6.7424,  5.3630,\n",
      "          4.8546,  3.5263],\n",
      "        [ 5.7000,  2.9270, 10.0762,  5.2805,  6.2261,  6.7816,  7.8900,  4.6669,\n",
      "          2.7988,  3.1375],\n",
      "        [ 7.3563,  7.6750,  3.6677,  3.5092,  2.8349,  1.8127,  6.5614,  3.3200,\n",
      "          7.5995,  7.1713],\n",
      "        [ 4.8375,  1.6027,  7.3421,  3.8845,  3.5906,  3.6627,  2.4417,  2.1184,\n",
      "          3.1200,  0.9055],\n",
      "        [ 5.3664,  9.3640,  6.6020,  4.3336,  6.7831,  7.9067,  9.1963,  6.7986,\n",
      "          6.2596,  5.0569],\n",
      "        [ 5.3401,  4.3880,  1.0314,  6.7830,  4.3413,  4.0417,  5.6371,  8.1345,\n",
      "          5.4640,  6.1549],\n",
      "        [ 2.0769,  1.2898,  6.2221,  1.9192,  5.4783,  1.7490,  3.6853,  3.4436,\n",
      "          6.6100,  5.5560],\n",
      "        [ 4.5380,  8.1702,  4.5182,  4.3441,  6.0467,  4.6641,  8.0400,  6.9073,\n",
      "          6.3135,  4.3672],\n",
      "        [ 5.2830,  5.1518,  4.6438,  0.9948,  7.7393,  9.5593,  8.3474,  3.8814,\n",
      "          7.3495,  3.8497],\n",
      "        [ 4.6541,  1.3795,  3.1709, -0.3117,  4.5367,  0.6725,  4.9326,  2.7298,\n",
      "          6.7797,  7.3981],\n",
      "        [ 6.2203,  3.0581,  8.0652,  5.7491,  5.2503,  4.3967,  4.2180,  3.5339,\n",
      "          3.4265,  4.0289],\n",
      "        [ 9.0654,  4.9816, -0.0469,  5.3196,  1.7252,  1.8978,  8.8254,  4.7669,\n",
      "          5.6519, 10.2040],\n",
      "        [ 2.6850,  4.6576,  1.8698,  3.8822,  5.6279,  8.9308,  4.6608,  4.7851,\n",
      "          4.3756,  6.3576],\n",
      "        [ 6.6207,  1.4000,  6.4253,  0.7730,  5.9208,  0.2003,  6.7521,  1.8070,\n",
      "          8.5528,  8.9632],\n",
      "        [ 1.6908,  3.5494,  3.1113,  3.1072,  3.8664,  7.8235,  0.4948,  1.4700,\n",
      "          6.5153,  4.5009],\n",
      "        [ 3.4904,  3.8324,  1.7064,  7.4893,  2.7400,  9.3235,  1.0816,  5.4620,\n",
      "          3.7461,  7.1037],\n",
      "        [ 6.8281,  6.3049,  9.8171,  6.6827,  5.8745,  4.3077,  7.0691,  6.0210,\n",
      "          4.9100,  6.7920],\n",
      "        [ 4.8297,  8.1051,  6.9201,  4.2779,  4.1177,  6.9249,  3.7428,  5.0950,\n",
      "          5.3239,  4.3880],\n",
      "        [ 4.3329,  6.8702,  7.4106,  5.5247,  5.5990,  2.9198,  7.6503,  6.3283,\n",
      "          5.7988,  5.8137],\n",
      "        [ 5.8160,  1.4986,  0.1340,  4.7460,  5.9194,  8.3720,  4.1152,  7.7076,\n",
      "          6.5260,  3.5220],\n",
      "        [ 4.3810,  4.2880,  1.3321,  4.9712,  2.5929,  2.7858,  4.6651,  4.6415,\n",
      "          4.5465,  6.0316],\n",
      "        [ 5.1359,  4.6569,  7.4417,  3.8030,  4.9888,  7.0409,  2.2870,  3.2877,\n",
      "          3.9571,  2.8559],\n",
      "        [ 4.7514,  7.3581,  6.5656,  5.3626,  4.2849,  2.6479,  7.4912,  5.7342,\n",
      "          5.7100,  6.3645],\n",
      "        [ 3.7294,  2.2027,  7.6136,  3.4754,  3.8541,  8.0235,  4.4701,  2.3075,\n",
      "          3.3175,  2.7748],\n",
      "        [ 2.9645,  6.4234,  3.9664,  2.1823,  4.9179,  7.5122,  0.9115,  2.4342,\n",
      "          6.2891,  4.1900],\n",
      "        [ 4.9725,  8.9721,  4.7317,  5.5441,  5.1758,  5.5222,  8.4261,  7.4953,\n",
      "          5.6972,  5.7612],\n",
      "        [ 2.1474,  5.7221,  2.9437,  3.0712,  5.4839,  8.2256,  0.1040,  1.0396,\n",
      "          7.7373,  4.4221],\n",
      "        [ 4.9114,  6.2986,  2.1039,  6.9245,  3.3150,  9.7247,  3.1709,  6.7479,\n",
      "          4.5640,  6.9912],\n",
      "        [ 6.6470,  7.3283,  3.5192,  3.3453,  2.2882,  3.2853,  3.7176,  3.0240,\n",
      "          6.7823,  5.2915],\n",
      "        [ 6.3910,  5.0291,  0.7190,  6.8186,  5.9952,  8.3914,  6.5887,  9.2239,\n",
      "          5.9808,  5.6574],\n",
      "        [ 5.3094,  4.3487,  1.0320,  7.9547,  4.3463, 10.0367,  5.6428,  8.8167,\n",
      "          3.7700,  7.4284],\n",
      "        [ 3.5033,  7.0361,  6.3184,  3.2471,  5.8363,  7.5224,  7.5253,  4.8492,\n",
      "          4.3898,  3.7815],\n",
      "        [ 2.4017,  4.8509,  5.1045,  2.4989,  4.2606,  6.8063,  3.7475,  2.9123,\n",
      "          3.9181,  3.2462],\n",
      "        [ 3.8768,  3.5901,  1.1119,  5.9334,  4.7721,  9.1274,  4.9928,  7.1282,\n",
      "          3.6898,  6.2313],\n",
      "        [ 5.5536,  4.9665,  1.3028,  5.0673,  6.5160,  4.1788,  6.0517,  8.2794,\n",
      "          7.0510,  3.5557],\n",
      "        [ 6.3634,  3.5056, 10.0906,  5.1739,  6.7021,  8.4047,  8.3142,  4.4875,\n",
      "          2.5358,  4.0092],\n",
      "        [ 5.7005,  1.9827,  8.2091,  5.9831,  5.4735,  3.6556,  5.1519,  3.4439,\n",
      "          3.0923,  4.0713],\n",
      "        [ 4.5354,  6.9436,  7.6501,  4.7845,  4.2400,  4.6956,  4.8626,  5.2007,\n",
      "          4.6441,  4.7133],\n",
      "        [ 2.0327,  3.8237,  1.6270,  4.8954,  3.1206,  4.9460,  3.8554,  5.2485,\n",
      "          2.6895,  4.0173],\n",
      "        [ 4.6102,  6.2792,  2.2437,  5.9552,  2.9565,  8.9484,  1.5482,  5.5730,\n",
      "          4.9389,  5.9778],\n",
      "        [ 6.5449, 10.0766,  6.5800,  4.9605,  4.0233,  8.6744,  3.9232,  5.9731,\n",
      "          6.7029,  5.5981],\n",
      "        [ 2.8998,  6.4394,  2.5376,  3.1706,  5.8303,  9.2094,  2.2693,  3.3566,\n",
      "          6.3370,  7.1214],\n",
      "        [ 4.1625,  1.5181,  7.9934,  5.4146,  5.6740,  2.3761,  5.3909,  3.7800,\n",
      "          4.0489,  4.5545],\n",
      "        [ 2.8824,  4.4102,  3.5041,  3.1622,  3.2879,  1.7378,  4.1762,  2.9066,\n",
      "          4.9223,  4.3270],\n",
      "        [ 2.4992,  3.0533,  2.1306,  7.1418,  3.8557, 11.6724, -0.3269,  2.9520,\n",
      "          7.1865,  8.4636],\n",
      "        [ 4.1477,  6.9237,  5.5386,  4.0623,  3.2705,  4.4708,  3.5902,  4.3950,\n",
      "          5.0595,  4.1765],\n",
      "        [ 5.3400,  5.0256,  7.5977,  4.8971,  4.6459,  2.3166,  6.0198,  3.9343,\n",
      "          5.7627,  6.2381],\n",
      "        [ 4.8481,  5.6390,  1.3293,  4.0688,  6.7641,  7.0211,  6.8275,  7.0827,\n",
      "          6.2456,  3.7256],\n",
      "        [ 2.9881,  7.4371,  5.1718,  3.6558,  4.9262,  7.6761,  6.5136,  4.7718,\n",
      "          4.1963,  4.2365],\n",
      "        [ 4.2190,  5.4550,  8.3901,  4.8687,  5.1909,  5.2838,  5.4739,  5.0838,\n",
      "          3.5687,  4.5478],\n",
      "        [ 3.6371,  4.0626,  7.5319,  5.0625,  5.7047,  2.7176,  6.3215,  4.9981,\n",
      "          4.8724,  5.2810],\n",
      "        [ 3.5662,  7.1136,  3.8073,  5.1260,  4.2175,  3.6146,  6.7402,  6.3130,\n",
      "          4.6586,  5.0477],\n",
      "        [ 7.0203,  8.3588,  2.3212,  6.9441,  3.9920,  3.4100,  9.0618,  8.0649,\n",
      "          6.5749,  8.3640]], grad_fn=<AddmmBackward0>)\n",
      "Latent Representation:\n",
      "tensor([[-2.3401, -0.2946,  0.9214,  0.8327],\n",
      "        [-1.2671, -0.5449,  2.0471, -0.2473],\n",
      "        [-3.0917,  0.4302,  1.9117,  0.0760],\n",
      "        [-2.9824,  0.1808,  0.4391,  2.1694],\n",
      "        [-2.1527, -0.6096,  0.7237,  0.3609],\n",
      "        [ 0.0850,  0.4779,  4.3886, -2.0169],\n",
      "        [-2.3969, -0.0301,  0.3863,  1.9226],\n",
      "        [-0.8959, -1.0645,  0.3657,  1.3588],\n",
      "        [-1.9961, -0.4208,  1.3703, -0.2036],\n",
      "        [-1.3676, -0.7284, -0.7123,  0.9245],\n",
      "        [-3.9641,  2.2359, -1.2801,  0.5752],\n",
      "        [-1.1152, -1.9884,  1.6656,  0.4429],\n",
      "        [-0.7503, -0.1134,  3.0562, -0.8967],\n",
      "        [-0.3241, -1.9871,  1.4934,  1.4564],\n",
      "        [-1.2344, -0.6250,  0.0089,  1.9295],\n",
      "        [-1.8743, -0.6777, -0.1348,  1.0989],\n",
      "        [-3.4256,  0.7554,  0.5141, -0.9394],\n",
      "        [-0.6600, -1.1122,  2.2870, -1.6610],\n",
      "        [-3.4276,  0.5163,  0.0989,  1.2786],\n",
      "        [-1.0653, -0.9228,  0.7025, -1.2133],\n",
      "        [-0.0962, -1.1790,  1.7759, -2.3401],\n",
      "        [-1.5813, -1.1624,  2.6614,  1.5085],\n",
      "        [-3.7775, -0.4092, -2.5215,  0.1883],\n",
      "        [-2.6207, -0.0341,  0.1222,  1.3491],\n",
      "        [-1.6451, -1.1667,  2.4970,  1.2557],\n",
      "        [-0.9536, -0.5120,  1.9801, -2.9502],\n",
      "        [ 0.3712, -2.9535,  1.0689, -0.4472],\n",
      "        [-0.9504,  0.6260,  1.8876,  1.2347],\n",
      "        [ 0.3562, -0.4782, -0.7085,  0.4687],\n",
      "        [-0.6870,  0.1850, -0.1891,  1.2176],\n",
      "        [-2.9763,  0.0441,  1.8411, -0.6357],\n",
      "        [-2.4861, -2.9888, -1.0021, -4.0409],\n",
      "        [-2.2671, -1.4991,  2.6047,  0.2102],\n",
      "        [-0.9069, -1.2572,  2.3638,  1.3887],\n",
      "        [-2.8523, -1.7600,  2.0641,  0.0337],\n",
      "        [-2.2070, -0.8315,  0.9356,  0.2695],\n",
      "        [-1.2020, -1.7226,  0.5918,  0.5560],\n",
      "        [-2.6931, -0.9457,  1.3207, -0.6734],\n",
      "        [-0.9532,  0.3553,  1.5855, -0.4334],\n",
      "        [-1.3756, -1.6526,  0.1226, -0.1326],\n",
      "        [-0.5456, -2.5260,  0.8363,  1.6650],\n",
      "        [-2.2364,  3.3485,  2.8030,  0.2951],\n",
      "        [-2.2268,  0.3616,  2.0218,  0.9055],\n",
      "        [-2.5285, -0.3222,  0.2505, -0.2122],\n",
      "        [-2.1131, -1.9173,  1.7985,  0.2831],\n",
      "        [-2.8331,  1.7916,  0.3124, -0.4555],\n",
      "        [ 0.8005, -2.8516, -0.7658,  1.1819],\n",
      "        [-1.4835, -1.5341,  0.8632,  0.2052],\n",
      "        [-0.5155, -1.8080,  2.8749, -1.9077],\n",
      "        [-3.1097,  1.1763,  1.2533,  1.0243],\n",
      "        [ 0.0878,  0.1919,  2.3964, -1.4596],\n",
      "        [-2.5994, -2.2944,  1.6139,  0.7625],\n",
      "        [-2.7679, -0.7749, -0.9871,  1.2826],\n",
      "        [-1.7310,  0.4855,  0.5014, -1.9183],\n",
      "        [-2.3674, -1.7403,  0.6938,  0.3912],\n",
      "        [-0.4745, -3.4216,  1.0441,  0.7838],\n",
      "        [-2.5014,  1.3278, -0.6782, -1.4033],\n",
      "        [-1.1171,  0.1252,  2.8659, -0.9221],\n",
      "        [-4.1652,  1.4731, -0.9917,  1.6792],\n",
      "        [-0.5645, -2.1005,  0.5347,  2.0668],\n",
      "        [-3.1169,  1.9633,  0.4232, -3.0752],\n",
      "        [ 0.8302, -1.0583,  0.7875,  2.1461],\n",
      "        [-0.9560, -0.4499, -0.2370,  3.4691],\n",
      "        [-2.5370, -0.5719,  2.7742, -0.5660],\n",
      "        [-1.4983, -0.2820,  2.1639,  1.4161],\n",
      "        [-2.6034, -0.7202,  1.4497, -0.3538],\n",
      "        [-0.9274, -2.6389, -1.7579,  1.6172],\n",
      "        [-2.2368,  0.3324, -0.2872,  1.0275],\n",
      "        [-0.3916, -0.3339,  2.7450,  0.6806],\n",
      "        [-2.5949, -0.3940,  1.2930,  0.0762],\n",
      "        [ 0.3389, -1.4993,  2.0446, -0.2661],\n",
      "        [ 0.1232, -0.4182,  1.7431,  2.2087],\n",
      "        [-2.8124, -1.5375,  1.2114,  0.9407],\n",
      "        [ 1.0784, -0.4684,  1.6304,  2.8150],\n",
      "        [-1.3958, -0.7286,  0.6099,  3.2084],\n",
      "        [-1.9927,  1.1217,  1.3111,  1.5270],\n",
      "        [-2.5424, -2.3715, -0.9459,  1.9485],\n",
      "        [-2.0769, -1.9204, -0.7377,  2.9340],\n",
      "        [-1.3042, -2.2215,  1.6150,  0.4367],\n",
      "        [-0.3197, -1.1894,  1.4006,  0.6893],\n",
      "        [-1.3273, -2.0374, -0.5273,  2.2645],\n",
      "        [-2.4911, -1.6477, -1.0872,  0.4496],\n",
      "        [-0.5869, -1.9874,  3.1481, -1.2833],\n",
      "        [-1.0256,  0.0209,  2.7931, -1.6443],\n",
      "        [-1.9382, -0.4508,  2.0021,  0.5132],\n",
      "        [-1.1543, -0.7511, -0.2004,  1.1243],\n",
      "        [-1.0307, -0.1787,  0.6316,  3.0475],\n",
      "        [-2.0108, -0.2032,  2.4442,  2.4625],\n",
      "        [-0.1380, -1.2535,  1.4427,  2.9869],\n",
      "        [-1.3499, -0.1729,  1.9898, -2.0802],\n",
      "        [-1.5651,  0.3143,  0.4192,  0.1837],\n",
      "        [ 0.8415, -1.9641, -0.2448,  4.0676],\n",
      "        [-1.4771, -0.0781,  1.7543,  1.1199],\n",
      "        [-2.2663,  0.2708,  2.1106, -0.5121],\n",
      "        [-1.7092, -2.2056, -0.2797,  1.1022],\n",
      "        [-1.2977, -1.7212,  1.3995,  1.0840],\n",
      "        [-1.5984, -0.9334,  2.0839, -0.3773],\n",
      "        [-1.9732, -0.4580,  1.6537, -1.2139],\n",
      "        [-2.1075, -0.9412,  0.5639,  0.5792],\n",
      "        [-3.8277, -0.4194,  0.1103,  1.3984]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Generate random data function\n",
    "def generate_data(size, factor_range=(0, 1)):\n",
    "    return torch.rand(size, input_dim) * (factor_range[1] - factor_range[0]) + factor_range[0]\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 10  # Number of features\n",
    "latent_dim = 4  # Dimension of the latent space\n",
    "\n",
    "# Generate training data\n",
    "train_data = generate_data(size=1000, factor_range=(0, 10))\n",
    "\n",
    "# Train the autoencoder\n",
    "model = Autoencoder(input_dim, latent_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i in range(len(train_data)):\n",
    "        # Get a mini-batch (consider using a DataLoader for larger datasets)\n",
    "        batch = train_data[i:i+batch_size]  # Replace 'batch_size' with your desired size\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed = model(batch)\n",
    "        loss = loss_fn(reconstructed, batch)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training progress (optional)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Generate data for augmentation\n",
    "new_data = generate_data(size=100, factor_range=(0, 10))  # Generate 100 data points\n",
    "\n",
    "# Assuming no preprocessing needed for this example\n",
    "processed_data = new_data\n",
    "\n",
    "# Get latent representation\n",
    "with torch.no_grad():\n",
    "    latent_representation = model.encoder(processed_data)\n",
    "\n",
    "# Augment data using latent space manipulation\n",
    "noise = torch.randn(latent_representation.size()) * 0.1  # Add small noise\n",
    "augmented_latent_rep = latent_representation + noise\n",
    "\n",
    "# Reconstruct data from augmented latent representation\n",
    "augmented_data = model.decoder(augmented_latent_rep)\n",
    "\n",
    "# Print original and augmented data (optional)\n",
    "print(\"Original Data:\")\n",
    "print(processed_data)\n",
    "print(\"Augmented Data:\")\n",
    "print(augmented_data)\n",
    "print('Latent Representation:')\n",
    "print(latent_representation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
